{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "263eXbWFF3hH",
        "outputId": "69fd8ded-a04a-4232-c929-d562440ae745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMDtU342DP8c"
      },
      "source": [
        "# Structured Streaming Basics\n",
        "Structured Streaming, as we discussed at the end of Chapter 20, is a stream processing\n",
        "framework built on the Spark SQL engine. Rather than introducing a separate API, Structured\n",
        "Streaming uses the existing structured APIs in Spark (DataFrames, Datasets, and SQL), meaning\n",
        "that all the operations you are familiar with there are supported. Users express a streaming\n",
        "computation in the same way they’d write a batch computation on static data. Upon specifying\n",
        "this, and specifying a streaming destination, the Structured Streaming engine will take care of\n",
        "running your query incrementally and continuously as new data arrives into the system.\n",
        "\n",
        "## Input Sources\n",
        "Structured Streaming supports several input sources for reading in a streaming fashion. As of\n",
        "Spark 2.2, the supported input sources are as follows:\n",
        "1. Apache Kafka 0.10\n",
        "2. Files on a distributed file system like HDFS or S3 (Spark will continuously read new\n",
        "files in a directory)\n",
        "3. A socket source for testing\n",
        "\n",
        "## Sinks\n",
        "Just as sources allow you to get data into Structured Streaming, sinks specify the destination for\n",
        "the result set of that stream. Sinks and the execution engine are also responsible for reliably\n",
        "tracking the exact progress of data processing. Here are the supported output sinks as of Spark 2.2:\n",
        "1. Apache Kafka 0.10\n",
        "2. Almost any file format\n",
        "3. A foreach sink for running arbitary computation on the output records\n",
        "4. A console sink for testing\n",
        "5. A memory sink for debugging\n",
        "\n",
        "## Output Modes\n",
        "Defining a sink for our Structured Streaming job is only half of the story. We also need to define\n",
        "how we want Spark to write data to that sink. For instance, do we only want to append new\n",
        "information? Do we want to update rows as we receive more information about them over time\n",
        "(e.g., updating the click count for a given web page)? Do we want to completely overwrite the\n",
        "result set every single time (i.e. always write a file with the complete click counts for all pages)?\n",
        "\n",
        "To do this, we define an output mode, similar to how we define output modes in the static\n",
        "Structured APIs.\n",
        "The supported output modes are as follows:\n",
        "1. Append (only add new records to the output sink)\n",
        "2. Update (update changed records in place)\n",
        "3. Complete (rewrite the full output)\n",
        "\n",
        "One important detail is that certain queries, and certain sinks, only support certain output modes,\n",
        "as we will discuss later in the book. For example, suppose that your job is just performing a map\n",
        "on a stream. The output data will grow indefinitely as new records arrive, so it would not make\n",
        "sense to use Complete mode, which requires writing all the data to a new file at once. In contrast,\n",
        "if you are doing an aggregation into a limited number of keys, Complete and Update modes\n",
        "would make sense, but Append would not, because the values of some keys’ need to be updated\n",
        "over time.\n",
        "\n",
        "## Triggers\n",
        "Whereas output modes define how data is output, triggers define when data is output—that is,\n",
        "when Structured Streaming should check for new input data and update its result. By default,\n",
        "Structured Streaming will look for new input records as soon as it has finished processing the last\n",
        "group of input data, giving the lowest latency possible for new results. However, this behavior\n",
        "can lead to writing many small output files when the sink is a set of files. Thus, Spark also\n",
        "supports triggers based on processing time (only look for new data at a fixed interval). In the\n",
        "future, other types of triggers may also be supported.\n",
        "\n",
        "### Event-Time Processing\n",
        "Structured Streaming also has support for event-time processing (i.e., processing data based on\n",
        "timestamps included in the record that may arrive out of order). There are two key ideas that you\n",
        "will need to understand here for the moment; we will talk about both of these in much more\n",
        "depth in the next chapter, so don’t worry if you’re not perfectly clear on them at this point.\n",
        "\n",
        "#### Watermarks\n",
        "Watermarks are a feature of streaming systems that allow you to specify how late they expect to\n",
        "see data in event time. For example, in an application that processes logs from mobile devices,\n",
        "one might expect logs to be up to 30 minutes late due to upload delays.\n",
        "\n",
        "# Structured Streaming in Action\n",
        "Let’s get to an applied example of how you might use Structured Streaming. For our examples,\n",
        "we’re going to be working with the Heterogeneity Human Activity Recognition Dataset. The\n",
        "data consists of smartphone and smartwatch sensor readings from a variety of devices—\n",
        "specifically, the accelerometer and gyroscope, sampled at the highest possible frequency\n",
        "supported by the devices. Readings from these sensors were recorded while users performed\n",
        "activities like biking, sitting, standing, walking, and so on. There are several different\n",
        "smartphones and smartwatches used, and nine total users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDiO2lO_F8xd",
        "outputId": "48ab13f5-d84c-45d3-85f5-d4bb21bd7a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OIltT8ttDP8f"
      },
      "outputs": [],
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q8GYcY26DP8g"
      },
      "outputs": [],
      "source": [
        "#static = spark.read.json(\"../data/activity-data/\")\n",
        "static = spark.read.json(\"/content/drive/MyDrive/UET/BigData/Lab07/data/flight-data/json/\")\n",
        "\n",
        "dataSchema = static.schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psjJ0swfDP8g",
        "outputId": "4c478df6-f995-4080-8cc3-999f8458cedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "static.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ltECLQyDP8g",
        "outputId": "b37d2b91-52eb-4909-86e1-30ae0d984b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "static.show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eacaPxsrDP8h"
      },
      "source": [
        "Next, let’s create a streaming version of the same Dataset, which will read each input file in the\n",
        "dataset one by one as if it was a stream.\n",
        "Streaming DataFrames are largely the same as static DataFrames.\n",
        "\n",
        "However, one small difference is that Structured Streaming does not let\n",
        "you perform schema inference without explicitly enabling it. You can enable schema inference\n",
        "for this by setting the configuration spark.sql.streaming.schemaInference to true. Given\n",
        "that fact, we will read the schema from one file (that we know has a valid schema) and pass the\n",
        "dataSchema object from our static DataFrame to our streaming DataFrame. As mentioned, you\n",
        "should avoid doing this in a production scenario where your data may (accidentally) change out\n",
        "from under you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TDrGxDznDP8h"
      },
      "outputs": [],
      "source": [
        "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
        ".json(\"/content/drive/MyDrive/UET/BigData/Lab07/data/flight-data/json/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHQBAMnHHOri"
      },
      "outputs": [],
      "source": [
        "# streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
        "# .json(\"/content/drive/MyDrive/UET/BigData/Lab07/data/activity-data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCSI3YonDP8h"
      },
      "source": [
        "Just like with other Spark APIs, streaming DataFrame creation and execution is lazy. In\n",
        "particular, we can now specify transformations on our streaming DataFrame before finally\n",
        "calling an action to start the stream. In this case, we’ll show one simple transformation—we will\n",
        "group and count data by the gt column, which is the activity being performed by the user at that\n",
        "point in time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fch6K0HADP8h"
      },
      "outputs": [],
      "source": [
        "activityCounts = streaming.groupBy(\"ORIGIN_COUNTRY_NAME\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLFcx9VXDP8h"
      },
      "source": [
        "Because this code is being written in local mode on a small machine, we are going to set the\n",
        "shuffle partitions to a small value to avoid creating too many shuffle partitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D3NhAdI8DP8h"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUVlYoP-DP8i"
      },
      "source": [
        "Now that we set up our transformation, we need only to specify our action to start the query. As\n",
        "mentioned previously in the chapter, we will specify an output destination, or output sink for our\n",
        "result of this query. For this basic example, we are going to write to a memory sink which keeps\n",
        "an in-memory table of the results.\n",
        "\n",
        "In the process of specifying this sink, we’re going to need to define how Spark will output that\n",
        "data. In this example, we use the complete output mode. This mode rewrites all of the keys along\n",
        "with their counts after every trigger:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Lu4EZSDGDP8i"
      },
      "outputs": [],
      "source": [
        "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
        ".format(\"memory\").outputMode(\"complete\")\\\n",
        ".start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4W0RvTJDP8i"
      },
      "source": [
        "We are now writing out our stream! You’ll notice that we set a unique query name to represent\n",
        "this stream, in this case activity_counts. We specified our format as an in-memory table and\n",
        "we set the output mode.\n",
        "\n",
        "When we run the preceding code, we also want to include the following line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oJX30-QpDP8i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "62bd81a7-0674-4869-a54a-172ff10566fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c406b906d45f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactivityQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "activityQuery.awaitTermination(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpsEGTbqDP8i"
      },
      "source": [
        "After this code is executed, the streaming computation will have started in the background. The\n",
        "query object is a handle to that active streaming query, and we must specify that we would like\n",
        "to wait for the termination of the query using activityQuery.awaitTermination() to prevent\n",
        "the driver process from exiting while the query is active. We will omit this from our future parts\n",
        "of the book for readability, but it must be included in your production applications; otherwise,\n",
        "your stream won’t be able to run.\n",
        "\n",
        "Spark lists this stream, and other active ones, under the active streams in our SparkSession. We\n",
        "can see a list of those streams by running the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IoUBcrM6DP8i",
        "outputId": "495944f3-b5e3-412d-96b8-b1a912ba8473",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<pyspark.sql.streaming.query.StreamingQuery at 0x7ed443139510>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "list(spark.streams.active)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0ksMC5PDP8i",
        "outputId": "4e612738-78a2-4ab1-b74b-0286a7b761c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "| ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-----+\n",
            "|             Ireland|    6|\n",
            "|            Kiribati|    6|\n",
            "|            Malaysia|    4|\n",
            "|               Japan|    6|\n",
            "|             Belgium|    6|\n",
            "|             Nigeria|    6|\n",
            "|           Indonesia|    4|\n",
            "|        Saint Martin|    2|\n",
            "|Bonaire, Sint Eus...|    6|\n",
            "|           Singapore|    5|\n",
            "|             Algeria|    2|\n",
            "|            Portugal|    6|\n",
            "|   Equatorial Guinea|    1|\n",
            "|              France|    6|\n",
            "|             Bolivia|    6|\n",
            "|         New Zealand|    6|\n",
            "|            Cambodia|    3|\n",
            "|         Afghanistan|    3|\n",
            "|Turks and Caicos ...|    6|\n",
            "|    Marshall Islands|    6|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+-----+\n",
            "| ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-----+\n",
            "|             Ireland|    6|\n",
            "|            Kiribati|    6|\n",
            "|            Malaysia|    4|\n",
            "|               Japan|    6|\n",
            "|             Belgium|    6|\n",
            "|             Nigeria|    6|\n",
            "|           Indonesia|    4|\n",
            "|        Saint Martin|    2|\n",
            "|Bonaire, Sint Eus...|    6|\n",
            "|           Singapore|    5|\n",
            "|             Algeria|    2|\n",
            "|            Portugal|    6|\n",
            "|   Equatorial Guinea|    1|\n",
            "|              France|    6|\n",
            "|             Bolivia|    6|\n",
            "|         New Zealand|    6|\n",
            "|            Cambodia|    3|\n",
            "|         Afghanistan|    3|\n",
            "|Turks and Caicos ...|    6|\n",
            "|    Marshall Islands|    6|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+-----+\n",
            "| ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-----+\n",
            "|             Ireland|    6|\n",
            "|            Kiribati|    6|\n",
            "|            Malaysia|    4|\n",
            "|               Japan|    6|\n",
            "|             Belgium|    6|\n",
            "|             Nigeria|    6|\n",
            "|           Indonesia|    4|\n",
            "|        Saint Martin|    2|\n",
            "|Bonaire, Sint Eus...|    6|\n",
            "|           Singapore|    5|\n",
            "|             Algeria|    2|\n",
            "|            Portugal|    6|\n",
            "|   Equatorial Guinea|    1|\n",
            "|              France|    6|\n",
            "|             Bolivia|    6|\n",
            "|         New Zealand|    6|\n",
            "|            Cambodia|    3|\n",
            "|         Afghanistan|    3|\n",
            "|Turks and Caicos ...|    6|\n",
            "|    Marshall Islands|    6|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+-----+\n",
            "| ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-----+\n",
            "|             Ireland|    6|\n",
            "|            Kiribati|    6|\n",
            "|            Malaysia|    4|\n",
            "|               Japan|    6|\n",
            "|             Belgium|    6|\n",
            "|             Nigeria|    6|\n",
            "|           Indonesia|    4|\n",
            "|        Saint Martin|    2|\n",
            "|Bonaire, Sint Eus...|    6|\n",
            "|           Singapore|    5|\n",
            "|             Algeria|    2|\n",
            "|            Portugal|    6|\n",
            "|   Equatorial Guinea|    1|\n",
            "|              France|    6|\n",
            "|             Bolivia|    6|\n",
            "|         New Zealand|    6|\n",
            "|            Cambodia|    3|\n",
            "|         Afghanistan|    3|\n",
            "|Turks and Caicos ...|    6|\n",
            "|    Marshall Islands|    6|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+-----+\n",
            "| ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-----+\n",
            "|             Ireland|    6|\n",
            "|            Kiribati|    6|\n",
            "|            Malaysia|    4|\n",
            "|               Japan|    6|\n",
            "|             Belgium|    6|\n",
            "|             Nigeria|    6|\n",
            "|           Indonesia|    4|\n",
            "|        Saint Martin|    2|\n",
            "|Bonaire, Sint Eus...|    6|\n",
            "|           Singapore|    5|\n",
            "|             Algeria|    2|\n",
            "|            Portugal|    6|\n",
            "|   Equatorial Guinea|    1|\n",
            "|              France|    6|\n",
            "|             Bolivia|    6|\n",
            "|         New Zealand|    6|\n",
            "|            Cambodia|    3|\n",
            "|         Afghanistan|    3|\n",
            "|Turks and Caicos ...|    6|\n",
            "|    Marshall Islands|    6|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from time import sleep\n",
        "for x in range(5):\n",
        "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
        "    sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBLQomlqDP8i"
      },
      "source": [
        "# Transformations on Streams\n",
        "Streaming transformations, as we mentioned, include almost all static DataFrame\n",
        "transformations that you already saw in Part II. All select, filter, and simple transformations are\n",
        "supported, as are all DataFrame functions and individual column manipulations. The limitations\n",
        "arise on transformations that do not make sense in context of streaming data. For example, as of\n",
        "Apache Spark 2.2, users cannot sort streams that are not aggregated, and cannot perform\n",
        "multiple levels of aggregation without using Stateful Processing (covered in the next chater).\n",
        "These limitations may be lifted as Structured Streaming continues to develop, so we encourage\n",
        "you to check the documentation of your version of Spark for updates.\n",
        "\n",
        "## Selections and Filtering\n",
        "All select and filter transformations are supported in Structured Streaming, as are all DataFrame\n",
        "functions and individual column manipulations. We show a simple example using selections and\n",
        "filtering below. In this case, because we are not updating any keys over time, we will use the\n",
        "Append output mode, so that new results are appended to the output table:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "static2 = spark.read.json(\"/content/drive/MyDrive/UET/BigData/Lab07/data/activity-data\")\n",
        "\n",
        "dataSchema2 = static2.schema"
      ],
      "metadata": {
        "id": "kLGqlK2NPNAX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streaming2 = spark.readStream.schema(dataSchema2).option(\"maxFilesPerTrigger\", 1)\\\n",
        ".json(\"/content/drive/MyDrive/UET/BigData/Lab07/data/activity-data\")"
      ],
      "metadata": {
        "id": "2xsEjDUOH5-R"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wYr_g9xqDP8i"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "simpleTransform = streaming2.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
        ".where(\"stairs\")\\\n",
        ".where(\"gt is not null\")\\\n",
        ".select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
        ".writeStream\\\n",
        ".queryName(\"simple_transform\")\\\n",
        ".format(\"memory\")\\\n",
        ".outputMode(\"append\")\\\n",
        ".start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P0kcQoHDP8j"
      },
      "source": [
        "## Aggregations\n",
        "Structured Streaming has excellent support for aggregations. You can specify arbitrary\n",
        "aggregations, as you saw in the Structured APIs. For example, you can use a more exotic\n",
        "aggregation, like a cube, on the phone model and activity and the average x, y, z accelerations of\n",
        "our sensor (jump back to Chapter 7 in order to see potential aggregations that you can run on\n",
        "your stream):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aPBEtqZADP8j"
      },
      "outputs": [],
      "source": [
        "deviceModelStats = streaming2.cube(\"gt\", \"model\").avg()\\\n",
        ".drop(\"avg(Arrival_time)\")\\\n",
        ".drop(\"avg(Creation_Time)\")\\\n",
        ".drop(\"avg(Index)\")\\\n",
        ".writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
        ".outputMode(\"complete\")\\\n",
        ".start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RGvGQVAFDP8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5140155-229a-4e89-cdc1-078a80239a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+--------------------+--------------------+--------------------+\n",
            "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
            "+----------+------+--------------------+--------------------+--------------------+\n",
            "|       sit|  NULL|-5.29444804083026...|2.306038601533980...|-1.78822936287358...|\n",
            "|      walk|nexus4|-0.00420826299253...|0.002500106691154...|-6.60631314217389...|\n",
            "|      walk|  NULL|-0.00420826299253...|0.002500106691154...|-6.60631314217389...|\n",
            "|  stairsup|  NULL|-0.02578427338191381|-0.00965934927050...|-0.09839209486330742|\n",
            "|     stand|  NULL|-2.91591635747158...|3.238512212841073E-4|2.023289649408601...|\n",
            "|      bike|  NULL|0.023786386015610356|-0.01037095838702...|-0.08199840483895786|\n",
            "|  stairsup|nexus4|-0.02578427338191381|-0.00965934927050...|-0.09839209486330742|\n",
            "|      NULL|nexus4|6.965503612181018E-4|-0.00697587517622801|-0.00907852353383...|\n",
            "|      NULL|  NULL|6.965503612181018E-4|-0.00697587517622801|-0.00907852353383...|\n",
            "|stairsdown|  NULL|0.023769356482889324|-0.03958739158701669| 0.12432091986134063|\n",
            "|      null|  NULL|-0.00860327075971...|-2.27032223826671...|0.004064154599349188|\n",
            "|       sit|nexus4|-5.29444804083026...|2.306038601533980...|-1.78822936287358...|\n",
            "|stairsdown|nexus4|0.023769356482889324|-0.03958739158701669| 0.12432091986134063|\n",
            "|     stand|nexus4|-2.91591635747158...|3.238512212841073E-4|2.023289649408601...|\n",
            "|      null|nexus4|-0.00860327075971...|-2.27032223826671...|0.004064154599349188|\n",
            "|      bike|nexus4|0.023786386015610356|-0.01037095838702...|-0.08199840483895786|\n",
            "+----------+------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT * FROM device_counts').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuEXyTrjDP8k"
      },
      "source": [
        "In addition to these aggregations on raw columns in the dataset, Structured Streaming has special\n",
        "support for columns that represent event time, including watermark support and windowing. We\n",
        "will discuss these in more detail in Chapter 22."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekj9qsI6DP8k"
      },
      "source": [
        "## Joins\n",
        "As of Apache Spark 2.2, Structured Streaming supports joining streaming DataFrames to static\n",
        "DataFrames. Spark 2.3 will add the ability to join multiple streams together. You can do multiple\n",
        "column joins and supplement streaming data with that from static data sources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6m92uu6EDP8k"
      },
      "outputs": [],
      "source": [
        "historicalAgg = static2.groupBy(\"gt\", \"model\").avg()\n",
        "deviceModelStats = streaming2.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n",
        ".cube(\"gt\", \"model\").avg()\\\n",
        ".join(historicalAgg, [\"gt\", \"model\"])\\\n",
        ".writeStream.queryName(\"device_counts_1\").format(\"memory\")\\\n",
        ".outputMode(\"complete\")\\\n",
        ".start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYqrz66yDP8k"
      },
      "source": [
        "In Spark 2.2, full outer joins, left joins with the stream on the right side, and right joins with the\n",
        "stream on the left are not supported. Structured Streaming also does not yet support stream-tostream\n",
        "joins, but this is also a feature under active development.\n",
        "# Input and Output\n",
        "This section dives deeper into the details of how sources, sinks, and output modes work in\n",
        "Structured Streaming. Specifically, we discuss how, when, and where data flows into and out of\n",
        "the system.\n",
        "## File source and sink\n",
        "Probably the simplest source you can think of is the simple file source. It’s easy to reason about\n",
        "and understand. While essentially any file source should work, the ones that we see in practice\n",
        "are Parquet, text, JSON, and CSV.\n",
        "\n",
        "The only difference between using the file source/sink and Spark’s static file source is that with\n",
        "streaming, we can control the number of files that we read in during each trigger via the\n",
        "maxFilesPerTrigger option that we saw earlier.\n",
        "\n",
        "Keep in mind that any files you add into an input directory for a streaming job need to appear in\n",
        "it atomically. Otherwise, Spark will process partially written files before you have finished. On\n",
        "file systems that show partial writes, such as local files or HDFS, this is best done by writing the\n",
        "file in an external directory and moving it into the input directory when finished. On Amazon S3,\n",
        "objects normally only appear once fully written.\n",
        "\n",
        "## Kafka source and sink\n",
        "Apache Kafka is a distributed publish-and-subscribe system for streams of data. Kafka lets you\n",
        "publish and subscribe to streams of records like you might do with a message queue—these are\n",
        "stored as streams of records in a fault-tolerant way. Think of Kafka like a distributed buffer.\n",
        "\n",
        "Kafka lets you store streams of records in categories that are referred to as topics. Each record in\n",
        "Kafka consists of a key, a value, and a timestamp. Topics consist of immutable sequences of\n",
        "records for which the position of a record in a sequence is called an offset. Reading data is called\n",
        "subscribing to a topic and writing data is as simple as publishing to a topic.\n",
        "Spark allows you to read from Kafka with both batch and streaming DataFrames.\n",
        "\n",
        "To read from Kafka, do the following in Structured Streaming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "arbi6RcbDP8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "b77ab794-9ee5-44d7-998d-6d6e1fa492bb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-17105c0fb324>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"host1:port1,host2:port2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subscribe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"topic1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Subscribe to multiple topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     def json(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
          ]
        }
      ],
      "source": [
        "# Subscribe to 1 topic\n",
        "df1 = spark.readStream.format(\"kafka\")\\\n",
        ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
        ".option(\"subscribe\", \"topic1\")\\\n",
        ".load()\n",
        "# Subscribe to multiple topics\n",
        "df2 = spark.readStream.format(\"kafka\")\\\n",
        ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
        ".option(\"subscribe\", \"topic1,topic2\")\\\n",
        ".load()\n",
        "# Subscribe to a pattern\n",
        "df3 = spark.readStream.format(\"kafka\")\\\n",
        ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
        ".option(\"subscribePattern\", \"topic.*\")\\\n",
        ".load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucDdkrP-DP8k"
      },
      "source": [
        "Each row in the source will have the following schema:\n",
        "1. key: binary\n",
        "2. value: binary\n",
        "3. topic: string\n",
        "4. partition: int\n",
        "5. offset: long\n",
        "6. timestamp: long\n",
        "\n",
        "Each message in Kafka is likely to be serialized in some way. Using native Spark functions in\n",
        "the Structured APIs, or a User-Defined Function (UDF), you can parse the message into a more\n",
        "structured format analysis. A common pattern is to use JSON or Avro to read and write to Kafka.\n",
        "\n",
        "## Writing to the Kafka Sink\n",
        "Writing to Kafka queries is largely the same as reading from them except for fewer parameters.\n",
        "You’ll still need to specify the Kafka bootstrap servers, but the only other option you will need to\n",
        "supply is either a column with the topic specification or supply that as an option. For example,\n",
        "the following writes are equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jOKnZ-OJDP8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "d15505b5-7dfe-4b98-b0f9-6c8af3e1295c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2853277e9916>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CAST(key AS STRING)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CAST(value AS STRING)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"host1:port1,host2:port2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/to/HDFS-compatible/dir\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
          ]
        }
      ],
      "source": [
        "df1.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
        ".writeStream\\\n",
        ".format(\"kafka\")\\\n",
        ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
        ".option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n",
        ".start()\n",
        "df1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
        ".writeStream\\\n",
        ".format(\"kafka\")\\\n",
        ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
        ".option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n",
        ".option(\"topic\", \"topic1\")\\\n",
        ".start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ElzDUUWDP8k"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38uJSXSXDP8k"
      },
      "source": [
        "## When Data Is Output (Triggers)\n",
        "To control when data is output to our sink, we set a trigger. By default, Structured Streaming\n",
        "will start data as soon as the previous trigger completes processing. You can use triggers to\n",
        "ensure that you do not overwhelm your output sink with too many updates or to try and control\n",
        "file sizes in the output. Currently, there is one periodic trigger type, based on processing time, as\n",
        "well as a “once” trigger to manually run a processing step once. More triggers will likely be\n",
        "added in the future.\n",
        "### Processing time trigger\n",
        "For the processing time trigger, we simply specify a duration as a string (you may also use a\n",
        "Duration in Scala or TimeUnit in Java). We’ll show the string format below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cWM0g51FDP8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc82366-0374-4636-bf33-28ce34167aeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7ed4430902e0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "activityCounts.writeStream.trigger(processingTime='5 seconds')\\\n",
        ".format(\"console\").outputMode(\"complete\").start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O5-d5bxDP8k"
      },
      "source": [
        "The ProcessingTime trigger will wait for multiples of the given duration in order to output data.\n",
        "For example, with a trigger duration of one minute, the trigger will fire at 12:00, 12:01, 12:02,\n",
        "and so on. If a trigger time is missed because the previous processing has not yet completed, then\n",
        "Spark will wait until the next trigger point (i.e., the next minute), rather than firing immediately\n",
        "after the previous processing completes.\n",
        "\n",
        "### Once trigger\n",
        "You can also just run a streaming job once by setting that as the trigger. This might seem like a\n",
        "weird case, but it’s actually extremely useful in both development and production. During\n",
        "development, you can test your application on just one trigger’s worth of data at a time. During\n",
        "production, the Once trigger can be used to run your job manually at a low rate (e.g., import new\n",
        "data into a summary table just occasionally). Because Structured Streaming still fully tracks all\n",
        "the input files processed and the state of the computation, this is easier than writing your own\n",
        "custom logic to track this in a batch job, and saves a lot of resources over running a continuous\n",
        "job 24/7:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "g8Gf_fy4DP8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58f2467-fd05-49ae-b622-be7ce7ff4e7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7ed442fa1ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "activityCounts.writeStream.trigger(once=True)\\\n",
        ".format(\"console\").outputMode(\"complete\").start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYW7UgonDP8k"
      },
      "source": [
        "# Conclusion\n",
        "It should be clear that Structured Streaming presents a powerful way to write streaming\n",
        "applications. Taking a batch job you already run and turning it into a streaming job with almost\n",
        "no code changes is both simple and extremely helpful from an engineering standpoint if you\n",
        "need to have this job interact closely with the rest of your data processing application.\n",
        "Chapter 22 dives into two advanced streaming-related concepts: event-time processing and\n",
        "stateful processing. Then, after that, Chapter 23 addresses what you need to do to run Structured\n",
        "Streaming in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZEhyvAwDP8l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}