{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSk7h-FTRX5d"
      },
      "source": [
        "# Formatting Models According to Your Use Case\n",
        "To preprocess data for Spark’s different advanced analytics tools, you must consider your end\n",
        "objective. The following list walks through the requirements for input data structure for each\n",
        "advanced analytics task in MLlib:\n",
        "1. In the case of most classification and regression algorithms, you want to get your data\n",
        "into a column of type Double to represent the label and a column of type Vector (either\n",
        "dense or sparse) to represent the features.\n",
        "2. In the case of recommendation, you want to get your data into a column of users, a\n",
        "column of items (say movies or books), and a column of ratings.\n",
        "3. In the case of unsupervised learning, a column of type Vector (either dense or sparse) is\n",
        "needed to represent the features.\n",
        "4. In the case of graph analytics, you will want a DataFrame of vertices and a DataFrame\n",
        "of edges.\n",
        "\n",
        "The best way to get your data in these formats is through transformers. Transformers are\n",
        "functions that accept a DataFrame as an argument and return a new DataFrame as a response.\n",
        "This chapter will focus on what transformers are relevant for particular use cases rather than\n",
        "attempting to enumerate every possible transformer.\n",
        "\n",
        "## NOTE\n",
        "Spark provides a number of transformers as part of the org.apache.spark.ml.feature package. The\n",
        "corresponding package in Python is pyspark.ml.feature. New transformers are constantly popping\n",
        "up in Spark MLlib and therefore it is impossible to include a definitive list in this book. The most upto-\n",
        "date information can be found on the Spark documentation site.\n",
        "\n",
        "Before we proceed, we’re going to read in several different sample datasets, each of which has\n",
        "different properties we will manipulate in this chapter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hmnr_QGtRX5e"
      },
      "outputs": [],
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext('local')\n",
        "sc.setSystemProperty('spark.executor.memory', '5g')\n",
        "sc.setSystemProperty('spark.driver.memory', '3g')\n",
        "\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvoCKMT_UliJ",
        "outputId": "178271af-02ab-44b2-ea57-c5790b3e60a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WmzE8HpoRX5e"
      },
      "outputs": [],
      "source": [
        "sales = spark.read.format(\"csv\")\\\n",
        ".option(\"header\", \"true\")\\\n",
        ".option(\"inferSchema\", \"true\")\\\n",
        ".load(\"drive/MyDrive/UET/BigData/Lab07/data/retail-data/by-day/*.csv\")\\\n",
        ".coalesce(5)\\\n",
        ".where(\"Description IS NOT NULL\")\n",
        "#fakeIntDF = spark.read.parquet(\"../data/simple-ml-integers\")\n",
        "simpleDF = spark.read.json(\"drive/MyDrive/UET/BigData/Lab07/data/simple-ml\")\n",
        "#scaleDF = spark.read.parquet(\"../data/simple-ml-scaling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z26bbKZARX5f"
      },
      "outputs": [],
      "source": [
        "scaleDF = spark.read.parquet(\"drive/MyDrive/UET/BigData/Lab07/data/simple-ml-scaling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bgZGpD24RX5f"
      },
      "outputs": [],
      "source": [
        "fakeIntDF = spark.read.parquet(\"drive/MyDrive/UET/BigData/Lab07/data/simple-ml-integers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuALV1QxRX5f"
      },
      "source": [
        "In addition to this realistic sales data, we’re going to use several simple synthetic datasets as\n",
        "well. FakeIntDF, simpleDF, and scaleDF all have very few rows. This will give you the ability\n",
        "to focus on the exact data manipulation we are performing instead of the various inconsistencies\n",
        "of any particular dataset. Because we’re going to be accessing the sales data a number of times,\n",
        "we’re going to cache it so we can read it efficiently from memory as opposed to reading it from\n",
        "disk every time we need it. Let’s also check out the first several rows of data in order to better\n",
        "understand what’s in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY8jltfkRX5f",
        "outputId": "a37a301e-c5bc-4149-e7b4-701a52de39eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
            "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
            "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
            "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
            "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
            "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
            "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
            "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
            "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
            "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
            "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
            "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
            "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
            "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
            "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
            "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
            "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sales.cache()\n",
        "sales.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxFYTjzGRX5f"
      },
      "source": [
        "## Transformers\n",
        "We discussed transformers in the previous chapter, but it’s worth reviewing them again here.\n",
        "Transformers are functions that convert raw data in some way. This might be to create a new\n",
        "interaction variable (from two other variables), to normalize a column, or to simply turn it into a\n",
        "Double to be input into a model. Transformers are primarily used in preprocessing or feature\n",
        "generation.\n",
        "\n",
        "Spark’s transformer only includes a transform method. This is because it will not change based\n",
        "on the input data.\n",
        "\n",
        "The Tokenizer is an example of a transformer. It tokenizes a string, splitting on a given\n",
        "character, and has nothing to learn from our data; it simply applies a function. We’ll discuss the\n",
        "tokenizer in more depth later in this chapter, but here’s a small code snippet showing how a\n",
        "tokenizer is built to accept the input column, how it transforms the data, and then the output from\n",
        "that transformation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X_1D5ugRX5g"
      },
      "outputs": [],
      "source": [
        "#// in Scala\n",
        "#import org.apache.spark.ml.feature.Tokenizer\n",
        "#val tkn = new Tokenizer().setInputCol(\"Description\")\n",
        "#tkn.transform(sales.select(\"Description\")).show(false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDiDR4ZNRX5g"
      },
      "source": [
        "## Estimators for Preprocessing\n",
        "Another tool for preprocessing are estimators. An estimator is necessary when a transformation\n",
        "you would like to perform must be initialized with data or information about the input column\n",
        "(often derived by doing a pass over the input column itself). For example, if you wanted to scale\n",
        "the values in our column to have mean zero and unit variance, you would need to perform a pass\n",
        "over the entire data in order to calculate the values you would use to normalize the data to mean\n",
        "zero and unit variance. In effect, an estimator can be a transformer configured according to your\n",
        "particular input data. In simplest terms, you can either blindly apply a transformation (a “regular”\n",
        "transformer type) or perform a transformation based on your data (an estimator type). Figure 25-\n",
        "2 is a simple illustration of an estimator fitting to a particular input dataset, generating a\n",
        "transformer that is then applied to the input dataset to append a new column (of the transformed\n",
        "data).\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "An example of this type of estimator is the StandardScaler, which scales your input column\n",
        "according to the range of values in that column to have a zero mean and a variance of 1 in each\n",
        "dimension. For that reason it must first perform a pass over the data to create the transformer.\n",
        "Here’s a sample code snippet showing the entire process, as well as the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njzVDIrGRX5g"
      },
      "outputs": [],
      "source": [
        "#// in Scala\n",
        "#import org.apache.spark.ml.feature.StandardScaler\n",
        "#val ss = new StandardScaler().setInputCol(\"features\")\n",
        "#ss.fit(scaleDF).transform(scaleDF).show(false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOxKc2qqRX5g"
      },
      "source": [
        "## Transformer Properties\n",
        "All transformers require you to specify, at a minimum, the inputCol and the outputCol, which\n",
        "represent the column name of the input and output, respectively. You set these with\n",
        "setInputCol and setOutputCol. There are some defaults (you can find these in the\n",
        "documentation), but it is a best practice to manually specify them yourself for clarity. In addition\n",
        "to input and output columns, all transformers have different parameters that you can tune\n",
        "(whenever we mention a parameter in this chapter you must set it with a set() method). In\n",
        "Python, we also have another method to set these values with keyword arguments to the object’s\n",
        "constructor. We exclude these from the examples in the next chapter for consistency. Estimators\n",
        "require you to fit the transformer to your particular dataset and then call transform on the\n",
        "resulting object.\n",
        "\n",
        "## NOTE\n",
        "Spark MLlib stores metadata about the columns it uses in each DataFrame as an attribute on the\n",
        "column itself. This allows it to properly store (and annotate) that a column of Doubles may actually\n",
        "represent a series of categorical variables instead of continuous values. However, metadata won’t show\n",
        "up when you print the schema or the DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utlSdzw1RX5g"
      },
      "source": [
        "# High-Level Transformers\n",
        "High-level transformers, such as the RFormula we saw in the previous chapter, allow you to\n",
        "concisely specify a number of transformations in one. These operate at a “high level”, and allow\n",
        "you to avoid doing data manipulations or transformations one by one. In general, you should try\n",
        "to use the highest level transformers you can, in order to minimize the risk of error and help you\n",
        "focus on the business problem instead of the smaller details of implementation. While this is not\n",
        "always possible, it’s a good objective.\n",
        "\n",
        "## RFormula\n",
        "The RFormula is the easiest transfomer to use when you have “conventionally” formatted data.\n",
        "Spark borrows this transformer from the R language to make it simple to declaratively specify a\n",
        "set of transformations for your data. With this transformer, values can be either numerical or\n",
        "categorical and you do not need to extract values from strings or manipulate them in any way.\n",
        "The RFormula will automatically handle categorical inputs (specified as strings) by performing\n",
        "something called one-hot encoding. In brief, one-hot encoding converts a set of values into a set\n",
        "of binary columns specifying whether or not the data point has each particular value (we’ll\n",
        "discuss one-hot encoding in more depth later in the chapter). With the RFormula, numeric\n",
        "columns will be cast to Double but will not be one-hot encoded. If the label column is of type\n",
        "String, it will be first transformed to Double with StringIndexer.\n",
        "\n",
        "### WARNING\n",
        "Automatic casting of numeric columns to Double without one-hot encoding has some important\n",
        "implications. If you have numerically valued categorical variables, they will only be cast to Double,\n",
        "implicitly specifying an order. It is important to ensure the input types correspond to the expected\n",
        "conversion. If you have categorical variables that really have no order relation, they should be cast to\n",
        "String. You can also manually index columns (see “Working with Categorical Features”).\n",
        "\n",
        "The RFormula allows you to specify your transformations in declarative syntax. It is simple to\n",
        "use once you understand the syntax. Currently, RFormula supports a limited subset of the R\n",
        "operators that in practice work quite well for simple transformations. The basic operators are:\n",
        "1. ~\n",
        "Separate target and terms\n",
        "2. +\n",
        "Concatenate terms; “+ 0” means removing the intercept (this means the y-intercept of the line\n",
        "that we will fit will be 0)\n",
        "3. -\n",
        "Remove a term; “- 1” means removing intercept (this means the y-intercept of the line that\n",
        "we will fit will be 0)\n",
        "4. :\n",
        "Interaction (multiplication for numeric values, or binarized categorical values)\n",
        "5. .\n",
        "All columns except the target/dependent variable\n",
        "\n",
        "RFormula also uses default columns of label and features to label, you guessed it, the label\n",
        "and the set of features that it outputs (for supervised machine learning). The models covered later\n",
        "on in this chapter by default require those column names, making it easy to pass the resulting\n",
        "transformed DataFrame into a model for training. If this doesn’t make sense yet, don’t worry—\n",
        "it’ll become clear once we actually start using models in later chapters.\n",
        "\n",
        "Let’s use RFormula in an example. In this case, we want to use all available variables (the .) and\n",
        "then specify an interaction between value1 and color and value2 and color as additional\n",
        "features to generate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPM60BH2RX5g",
        "outputId": "ee1765b0-42d9-4932-ed17-8c28a027a3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------+------------------+--------------------+-----+\n",
            "|color| lab|value1|            value2|            features|label|\n",
            "+-----+----+------+------------------+--------------------+-----+\n",
            "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
            "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
            "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
            "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
            "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
            "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
            "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
            "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
            "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
            "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
            "|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n",
            "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
            "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
            "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
            "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
            "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
            "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
            "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
            "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
            "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
            "+-----+----+------+------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import RFormula\n",
        "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
        "supervised.fit(simpleDF).transform(simpleDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1i28ssKRX5h"
      },
      "source": [
        "## SQL Transformers\n",
        "A SQLTransformer allows you to leverage Spark’s vast library of SQL-related manipulations\n",
        "just as you would a MLlib transformation. Any SELECT statement you can use in SQL is a valid\n",
        "transformation. The only thing you need to change is that instead of using the table name, you\n",
        "should just use the keyword TH IS. You might want to use SQLTransformer if you want to\n",
        "formally codify some DataFrame manipulation as a preprocessing step, or try different SQL\n",
        "expressions for features during hyperparameter tuning. Also note that the output of this\n",
        "transformation will be appended as a column to the output DataFrame.\n",
        "\n",
        "You might want to use an SQLTransformer in order to represent all of your manipulations on the\n",
        "very rawest form of your data so you can version different variations of manipulations as\n",
        "transformers. This gives you the benefit of building and testing varying pipelines, all by simply\n",
        "swapping out transformers. The following is a basic example of using SQLTransformer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAqAM94jRX5h",
        "outputId": "452801b8-7b86-4446-ac99-8cd3ea37f69f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------+----------+\n",
            "|sum(Quantity)|count(1)|CustomerID|\n",
            "+-------------+--------+----------+\n",
            "|          119|      62|   14452.0|\n",
            "|          440|     143|   16916.0|\n",
            "|          630|      72|   17633.0|\n",
            "|           34|       6|   14768.0|\n",
            "|         1542|      30|   13094.0|\n",
            "|          854|     117|   17884.0|\n",
            "|           97|      12|   16596.0|\n",
            "|          290|      98|   13607.0|\n",
            "|          541|      27|   14285.0|\n",
            "|          244|      31|   16561.0|\n",
            "|          756|      67|   15145.0|\n",
            "|           83|      13|   16858.0|\n",
            "|           56|       4|   13160.0|\n",
            "|         8873|      80|   16656.0|\n",
            "|          241|      43|   16212.0|\n",
            "|          258|      23|   13142.0|\n",
            "|           67|      14|   13811.0|\n",
            "|          569|      57|   12550.0|\n",
            "|           84|       4|   15160.0|\n",
            "|          954|      82|   15067.0|\n",
            "+-------------+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import SQLTransformer\n",
        "basicTransformation = SQLTransformer()\\\n",
        ".setStatement(\"\"\"\n",
        "SELECT sum(Quantity), count(*), CustomerID\n",
        "FROM __THIS__\n",
        "GROUP BY CustomerID\n",
        "\"\"\")\n",
        "basicTransformation.transform(sales).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy0rMu0mRX5h"
      },
      "source": [
        "## VectorAssembler\n",
        "The VectorAssembler is a tool you’ll use in nearly every single pipeline you generate. It helps\n",
        "concatenate all your features into one big vector you can then pass into an estimator. It’s used\n",
        "typically in the last step of a machine learning pipeline and takes as input a number of columns\n",
        "of Boolean, Double, or Vector. This is particularly helpful if you’re going to perform a number\n",
        "of manipulations using a variety of transformers and need to gather all of those results together.\n",
        "The output from the following code snippet will make it clear how this works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgcZY3ueRX5h",
        "outputId": "688718e6-aae0-4065-9737-a5a491f4f9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+------------------------------------+\n",
            "|int1|int2|int3|VectorAssembler_6bf85052c195__output|\n",
            "+----+----+----+------------------------------------+\n",
            "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
            "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
            "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
            "+----+----+----+------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
        "va.transform(fakeIntDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxlBYIsQRX5h"
      },
      "source": [
        "# Working with Continuous Features\n",
        "Continuous features are just values on the number line, from positive infinity to negative infinity.\n",
        "There are two common transformers for continuous features. First, you can convert continuous\n",
        "features into categorical features via a process called bucketing, or you can scale and normalize\n",
        "your features according to several different requirements. These transformers will only work on\n",
        "Double types, so make sure you’ve turned any other numerical values to Double:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8HXNruRJRX5h"
      },
      "outputs": [],
      "source": [
        "contDF = spark.range(20).selectExpr(\"cast(id as double)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-6bQClXRX5h"
      },
      "source": [
        "## Bucketing\n",
        "The most straightforward approach to bucketing or binning is using the Bucketizer. This will\n",
        "split a given continuous feature into the buckets of your designation. You specify how buckets\n",
        "should be created via an array or list of Double values. This is useful because you may want to\n",
        "simplify the features in your dataset or simplify their representations for interpretation later on.\n",
        "For example, imagine you have a column that represents a person’s weight and you would like to\n",
        "predict some value based on this information. In some cases, it might be simpler to create three\n",
        "buckets of “overweight,” “average,” and “underweight.”\n",
        "\n",
        "To specify the bucket, set its borders. For example, setting splits to 5.0, 10.0, 250.0 on our\n",
        "contDF will actually fail because we don’t cover all possible input ranges. When specifying your\n",
        "bucket points, the values you pass into splits must satisfy three requirements:\n",
        "1. The minimum value in your splits array must be less than the minimum value in your\n",
        "DataFrame.\n",
        "2. The maximum value in your splits array must be greater than the maximum value in\n",
        "your DataFrame.\n",
        "3. You need to specify at a minimum three values in the splits array, which creates two\n",
        "buckets.\n",
        "\n",
        "To cover all possible ranges, scala.Double.NegativeInfinity might be another split option,\n",
        "with scala.Double.PositiveInfinity to cover all possible ranges outside of the inner splits.\n",
        "In Python we specify this in the following way: float(\"inf\"), float(\"-inf\").\n",
        "\n",
        "In order to handle null or NaN values, we must specify the handleInvalid parameter as a\n",
        "certain value. We can either keep those values (keep), error or null, or skip those rows.\n",
        "Here’s an example of using bucketing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkWMfA2qRX5h",
        "outputId": "4828d8a8-1b75-4ea2-8e70-e632232e832e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------------------------+\n",
            "|  id|Bucketizer_ddc704d553be__output|\n",
            "+----+-------------------------------+\n",
            "| 0.0|                            0.0|\n",
            "| 1.0|                            0.0|\n",
            "| 2.0|                            0.0|\n",
            "| 3.0|                            0.0|\n",
            "| 4.0|                            0.0|\n",
            "| 5.0|                            1.0|\n",
            "| 6.0|                            1.0|\n",
            "| 7.0|                            1.0|\n",
            "| 8.0|                            1.0|\n",
            "| 9.0|                            1.0|\n",
            "|10.0|                            2.0|\n",
            "|11.0|                            2.0|\n",
            "|12.0|                            2.0|\n",
            "|13.0|                            2.0|\n",
            "|14.0|                            2.0|\n",
            "|15.0|                            2.0|\n",
            "|16.0|                            2.0|\n",
            "|17.0|                            2.0|\n",
            "|18.0|                            2.0|\n",
            "|19.0|                            2.0|\n",
            "+----+-------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
        "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
        "bucketer.transform(contDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54_xGIYbRX5h"
      },
      "source": [
        "In addition to splitting based on hardcoded values, another option is to split based on percentiles\n",
        "in our data. This is done with QuantileDiscretizer, which will bucket the values into userspecified\n",
        "buckets with the splits being determined by approximate quantiles values. For instance,\n",
        "the 90th quantile is the point in your data at which 90% of the data is below that value. You can\n",
        "control how finely the buckets should be split by setting the relative error for the approximate\n",
        "quantiles calculation using setRelativeError. Spark does this is by allowing you to specify the\n",
        "number of buckets you would like out of the data and it will split up your data accordingly. The\n",
        "following is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXlo6l8vRX5i",
        "outputId": "a1aa2cd1-e5e5-4711-a08b-bfe12a7143f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------------------------------------+\n",
            "|  id|QuantileDiscretizer_8b778656b50e__output|\n",
            "+----+----------------------------------------+\n",
            "| 0.0|                                     0.0|\n",
            "| 1.0|                                     0.0|\n",
            "| 2.0|                                     0.0|\n",
            "| 3.0|                                     1.0|\n",
            "| 4.0|                                     1.0|\n",
            "| 5.0|                                     1.0|\n",
            "| 6.0|                                     1.0|\n",
            "| 7.0|                                     2.0|\n",
            "| 8.0|                                     2.0|\n",
            "| 9.0|                                     2.0|\n",
            "|10.0|                                     2.0|\n",
            "|11.0|                                     3.0|\n",
            "|12.0|                                     3.0|\n",
            "|13.0|                                     3.0|\n",
            "|14.0|                                     3.0|\n",
            "|15.0|                                     4.0|\n",
            "|16.0|                                     4.0|\n",
            "|17.0|                                     4.0|\n",
            "|18.0|                                     4.0|\n",
            "|19.0|                                     4.0|\n",
            "+----+----------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import QuantileDiscretizer\n",
        "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\")\n",
        "fittedBucketer = bucketer.fit(contDF)\n",
        "fittedBucketer.transform(contDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqnv9rUQRX5i"
      },
      "source": [
        "## Advanced bucketing techniques\n",
        "The techniques descriubed here are the most common ways of bucketing data, but there are a\n",
        "number of other ways that exist in Spark today. All of these processes are the same from a data\n",
        "flow perspective: start with continuous data and place them in buckets so that they become\n",
        "categorical. Differences arise depending on the algorithm used to compute these buckets. The\n",
        "simple examples we just looked at are easy to intepret and work with, but more advanced\n",
        "techniques such as locality sensitivity hashing (LSH) are also available in MLlib.\n",
        "\n",
        "# Scaling and Normalization\n",
        "We saw how we can use bucketing to create groups out of continuous variables. Another\n",
        "common task is to scale and normalize continuous data. While not always necessary, doing so is\n",
        "usually a best practice. You might want to do this when your data contains a number of columns\n",
        "based on different scales.\n",
        "\n",
        "## StandardScaler\n",
        "The StandardScaler standardizes a set of features to have zero mean and a standard deviation\n",
        "of 1. The flag withStd will scale the data to unit standard deviation while the flag withMean\n",
        "(false by default) will center the data prior to scaling it.\n",
        "### WARNING\n",
        "Centering can be very expensive on sparse vectors because it generally turns them into dense vectors,\n",
        "so be careful before centering your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF2wHgk_RX5i",
        "outputId": "bf215489-42bc-4c80-8189-181c784fb9fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+-----------------------------------+\n",
            "| id|      features|StandardScaler_1d78758218b1__output|\n",
            "+---+--------------+-----------------------------------+\n",
            "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
            "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
            "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
            "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
            "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
            "+---+--------------+-----------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "sScaler = StandardScaler().setInputCol(\"features\")\n",
        "sScaler.fit(scaleDF).transform(scaleDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pofc8tcrRX5i"
      },
      "source": [
        "## MinMaxScaler\n",
        "The MinMaxScaler will scale the values in a vector (component wise) to the proportional values\n",
        "on a scale from a given min value to a max value. If you specify the minimum value to be 0 and\n",
        "the maximum value to be 1, then all the values will fall in between 0 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poZ5-ODeRX5i",
        "outputId": "58c2f92c-28cd-4c70-ab43-59265bdb2c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+---------------------------------+\n",
            "| id|      features|MinMaxScaler_8fd11dff7dc2__output|\n",
            "+---+--------------+---------------------------------+\n",
            "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
            "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
            "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
            "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
            "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
            "+---+--------------+---------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
        "fittedminMax = minMax.fit(scaleDF)\n",
        "fittedminMax.transform(scaleDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGfwb3WyRX5i"
      },
      "source": [
        "## MaxAbsScaler\n",
        "The max absolute scaler (MaxAbsScaler) scales the data by dividing each value by the maximum\n",
        "absolute value in this feature. All values therefore end up between −1 and 1. This transformer\n",
        "does not shift or center the data at all in the process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Uz8YzUWRX5i",
        "outputId": "f1c1fb3b-d186-4497-e147-5a70aaac1e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+---------------------------------+\n",
            "| id|      features|MaxAbsScaler_9f4180675451__output|\n",
            "+---+--------------+---------------------------------+\n",
            "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
            "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
            "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
            "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
            "|  1|[3.0,10.1,3.0]|                    [1.0,1.0,1.0]|\n",
            "+---+--------------+---------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import MaxAbsScaler\n",
        "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
        "fittedmaScaler = maScaler.fit(scaleDF)\n",
        "fittedmaScaler.transform(scaleDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ntSiCCIRX5i"
      },
      "source": [
        "## ElementwiseProduct\n",
        "The ElementwiseProduct allows us to scale each value in a vector by an arbitrary value. For\n",
        "example, given the vector below and the row “1, 0.1, -1” the output will be “10, 1.5, -20.”\n",
        "Naturally the dimensions of the scaling vector must match the dimensions of the vector inside\n",
        "the relevant column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj5eyoE8RX5i",
        "outputId": "9e0cca35-67c0-416c-f833-87e91c659c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+---------------------------------------+\n",
            "| id|      features|ElementwiseProduct_dd13530e0d51__output|\n",
            "+---+--------------+---------------------------------------+\n",
            "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
            "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
            "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
            "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
            "|  1|[3.0,10.1,3.0]|                      [30.0,151.5,60.0]|\n",
            "+---+--------------+---------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import ElementwiseProduct\n",
        "from pyspark.ml.linalg import Vectors\n",
        "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
        "scalingUp = ElementwiseProduct()\\\n",
        ".setScalingVec(scaleUpVec)\\\n",
        ".setInputCol(\"features\")\n",
        "scalingUp.transform(scaleDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtTC8OBNRX5j"
      },
      "source": [
        "## Normalizer\n",
        "The normalizer allows us to scale multidimensional vectors using one of several power norms,\n",
        "set through the parameter “p”. For example, we can use the Manhattan norm (or Manhattan\n",
        "distance) with p = 1, Euclidean norm with p = 2, and so on. The Manhattan distance is a measure\n",
        "of distance where you can only travel from point to point along the straight lines of an axis (like\n",
        "the streets in Manhattan).\n",
        "Here’s an example of using the Normalizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n6u-3lBRX5j",
        "outputId": "1c80ce24-bd71-4483-9e17-91f3c7c583a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+-------------------------------+\n",
            "| id|      features|Normalizer_1428bd1c7b4b__output|\n",
            "+---+--------------+-------------------------------+\n",
            "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
            "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
            "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
            "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
            "|  1|[3.0,10.1,3.0]|           [0.18633540372670...|\n",
            "+---+--------------+-------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Normalizer\n",
        "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
        "manhattanDistance.transform(scaleDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af0GtJQbRX5j"
      },
      "source": [
        "# Working with Categorical Features\n",
        "The most common task for categorical features is indexing. Indexing converts a categorical\n",
        "variable in a column to a numerical one that you can plug into machine learning algorithms.\n",
        "While this is conceptually simple, there are some catches that are important to keep in mind so\n",
        "that Spark can do this in a stable and repeatable manner.\n",
        "\n",
        "In general, we recommend re-indexing every categorical variable when pre-processing just for\n",
        "consistency’s sake. This can be helpful in maintaining your models over the long run as your\n",
        "encoding practices may change over time.\n",
        "\n",
        "## StringIndexer\n",
        "The simplest way to index is via the StringIndexer, which maps strings to different numerical\n",
        "IDs. Spark’s StringIndexer also creates metadata attached to the DataFrame that specify what\n",
        "inputs correspond to what outputs. This allows us later to get inputs back from their respective\n",
        "index values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXC_8OcKRX5j",
        "outputId": "f538ab8b-29db-4bde-c6b0-a9a08d2c6a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------+------------------+--------+\n",
            "|color| lab|value1|            value2|labelInd|\n",
            "+-----+----+------+------------------+--------+\n",
            "|green|good|     1|14.386294994851129|     1.0|\n",
            "| blue| bad|     8|14.386294994851129|     0.0|\n",
            "| blue| bad|    12|14.386294994851129|     0.0|\n",
            "|green|good|    15| 38.97187133755819|     1.0|\n",
            "|green|good|    12|14.386294994851129|     1.0|\n",
            "|green| bad|    16|14.386294994851129|     0.0|\n",
            "|  red|good|    35|14.386294994851129|     1.0|\n",
            "|  red| bad|     1| 38.97187133755819|     0.0|\n",
            "|  red| bad|     2|14.386294994851129|     0.0|\n",
            "|  red| bad|    16|14.386294994851129|     0.0|\n",
            "|  red|good|    45| 38.97187133755819|     1.0|\n",
            "|green|good|     1|14.386294994851129|     1.0|\n",
            "| blue| bad|     8|14.386294994851129|     0.0|\n",
            "| blue| bad|    12|14.386294994851129|     0.0|\n",
            "|green|good|    15| 38.97187133755819|     1.0|\n",
            "|green|good|    12|14.386294994851129|     1.0|\n",
            "|green| bad|    16|14.386294994851129|     0.0|\n",
            "|  red|good|    35|14.386294994851129|     1.0|\n",
            "|  red| bad|     1| 38.97187133755819|     0.0|\n",
            "|  red| bad|     2|14.386294994851129|     0.0|\n",
            "+-----+----+------+------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
        "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
        "idxRes.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mju8uT02RX5j"
      },
      "source": [
        "Keep in mind that the StringIndexer is an estimator that must be fit on the input data. This\n",
        "means it must see all inputs to select a mapping of inputs to IDs. If you train a StringIndexer\n",
        "on inputs “a,” “b,” and “c” and then go to use it against input “d,” it will throw an error by\n",
        "default. Another option is to skip the entire row if the input value was not a value seen during\n",
        "training. Going along with the previous example, an input value of “d” would cause that row to\n",
        "be skipped entirely. We can set this option before or after training the indexer or pipeline. More\n",
        "options may be added to this feature in the future but as of Spark 2.2, you can only skip or throw\n",
        "an error on invalid inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5s3WvjEgRX5j"
      },
      "outputs": [],
      "source": [
        "# valIndexer.setHandleInvalid(\"skip\")\n",
        "# valIndexer.fit(simpleDF).setHandleInvalid(\"skip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3I-WoFRRX5j"
      },
      "source": [
        "## Converting Indexed Values Back to Text\n",
        "When inspecting your machine learning results, you’re likely going to want to map back to the\n",
        "original values. Since MLlib classification models make predictions using the indexed values,\n",
        "this conversion is useful for converting model predictions (indices) back to the original\n",
        "categories. We can do this with IndexToString. You’ll notice that we do not have to input our\n",
        "value to the String key; Spark’s MLlib maintains this metadata for you. You can optionally\n",
        "specify the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrobKoC5RX5m",
        "outputId": "56205a2b-a16d-4ffc-97dd-65e764803631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------+------------------+--------+----------------------------------+\n",
            "|color| lab|value1|            value2|labelInd|IndexToString_737d40bf8167__output|\n",
            "+-----+----+------+------------------+--------+----------------------------------+\n",
            "|green|good|     1|14.386294994851129|     1.0|                              good|\n",
            "| blue| bad|     8|14.386294994851129|     0.0|                               bad|\n",
            "| blue| bad|    12|14.386294994851129|     0.0|                               bad|\n",
            "|green|good|    15| 38.97187133755819|     1.0|                              good|\n",
            "|green|good|    12|14.386294994851129|     1.0|                              good|\n",
            "|green| bad|    16|14.386294994851129|     0.0|                               bad|\n",
            "|  red|good|    35|14.386294994851129|     1.0|                              good|\n",
            "|  red| bad|     1| 38.97187133755819|     0.0|                               bad|\n",
            "|  red| bad|     2|14.386294994851129|     0.0|                               bad|\n",
            "|  red| bad|    16|14.386294994851129|     0.0|                               bad|\n",
            "|  red|good|    45| 38.97187133755819|     1.0|                              good|\n",
            "|green|good|     1|14.386294994851129|     1.0|                              good|\n",
            "| blue| bad|     8|14.386294994851129|     0.0|                               bad|\n",
            "| blue| bad|    12|14.386294994851129|     0.0|                               bad|\n",
            "|green|good|    15| 38.97187133755819|     1.0|                              good|\n",
            "|green|good|    12|14.386294994851129|     1.0|                              good|\n",
            "|green| bad|    16|14.386294994851129|     0.0|                               bad|\n",
            "|  red|good|    35|14.386294994851129|     1.0|                              good|\n",
            "|  red| bad|     1| 38.97187133755819|     0.0|                               bad|\n",
            "|  red| bad|     2|14.386294994851129|     0.0|                               bad|\n",
            "+-----+----+------+------------------+--------+----------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import IndexToString\n",
        "labelReverse = IndexToString().setInputCol(\"labelInd\")\n",
        "labelReverse.transform(idxRes).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0DF1lzKRX5m"
      },
      "source": [
        "## Indexing in Vectors\n",
        "VectorIndexer is a helpful tool for working with categorical variables that are already found\n",
        "inside of vectors in your dataset. This tool will automatically find categorical features inside of\n",
        "your input vectors and convert them to categorical features with zero-based category indices. For\n",
        "example, in the following DataFrame, the first column in our Vector is a categorical variable\n",
        "with two different categories while the rest of the variables are continuous. By setting\n",
        "maxCategories to 2 in our VectorIndexer, we are instructing Spark to take any column in our\n",
        "vector with two or less distinct values and convert it to a categorical variable. This can be helpful\n",
        "when you know how many unique values there are in your largest category because you can\n",
        "specify this and it will automatically index the values accordingly. Conversely, Spark changes\n",
        "the data based on this parameter, so if you have continuous variables that don’t appear\n",
        "particularly continuous (lots of repeated values) these can be unintentionally converted to\n",
        "categorical variables if there are too few unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oex78mmDRX5m",
        "outputId": "3dce8c4f-fff5-4122-ba07-dd5d1c1d73cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+-------------+\n",
            "|     features|label|        idxed|\n",
            "+-------------+-----+-------------+\n",
            "|[1.0,2.0,3.0]|    1|[0.0,2.0,3.0]|\n",
            "|[2.0,5.0,6.0]|    2|[1.0,5.0,6.0]|\n",
            "|[1.0,8.0,9.0]|    3|[0.0,8.0,9.0]|\n",
            "+-------------+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorIndexer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "idxIn = spark.createDataFrame([\n",
        "(Vectors.dense(1, 2, 3),1),\n",
        "(Vectors.dense(2, 5, 6),2),\n",
        "(Vectors.dense(1, 8, 9),3)\n",
        "]).toDF(\"features\", \"label\")\n",
        "indxr = VectorIndexer()\\\n",
        ".setInputCol(\"features\")\\\n",
        ".setOutputCol(\"idxed\")\\\n",
        ".setMaxCategories(2)\n",
        "indxr.fit(idxIn).transform(idxIn).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ3QCFCsRX5m"
      },
      "source": [
        "## One-Hot Encoding\n",
        "Indexing categorical variables is only half of the story. One-hot encoding is an extremely\n",
        "common data transformation performed after indexing categorical variables. This is because\n",
        "indexing does not always represent our categorical variables in the correct way for downstream\n",
        "models to process. For instance, when we index our “color” column, you will notice that some\n",
        "colors have a higher value (or index number) than others (in our case, blue is 1 and green is 2).\n",
        "\n",
        "This is incorrect because it gives the mathematical appearance that the input to the machine\n",
        "learning algorithm seems to specify that green > blue, which makes no sense in the case of the\n",
        "current categories. To avoid this, we use OneHotEncoder, which will convert each distinct value\n",
        "to a Boolean flag (1 or 0) as a component in a vector. When we encode the color value, then we\n",
        "can see these are no longer ordered, making them easier for downstream models (e.g., a linear\n",
        "model) to process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT6tKLvgRX5m",
        "outputId": "cdae830e-4b09-4e98-c4a3-83a836f8023d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+----------------------------------+\n",
            "|color|colorInd|OneHotEncoder_8d585946eb63__output|\n",
            "+-----+--------+----------------------------------+\n",
            "|green|     1.0|                     (2,[1],[1.0])|\n",
            "| blue|     2.0|                         (2,[],[])|\n",
            "| blue|     2.0|                         (2,[],[])|\n",
            "|green|     1.0|                     (2,[1],[1.0])|\n",
            "|green|     1.0|                     (2,[1],[1.0])|\n",
            "|green|     1.0|                     (2,[1],[1.0])|\n",
            "|  red|     0.0|                     (2,[0],[1.0])|\n",
            "|  red|     0.0|                     (2,[0],[1.0])|\n",
            "|  red|     0.0|                     (2,[0],[1.0])|\n",
            "|  red|     0.0|                     (2,[0],[1.0])|\n",
            "|  red|     0.0|                     (2,[0],[1.0])|\n",
            "|green|     1.0|                     (2,[1],[1.0])|\n",
            "| blue|     2.0|                         (2,[],[])|\n",
            "| blue|     2.0|                         (2,[],[])|\n",
            "|green|     1.0|                     (2,[1],[1.0])|\n",
            "|green|     1.0|                     (2,[1],[1.0])|\n",
            "|green|     1.0|                     (2,[1],[1.0])|\n",
            "|  red|     0.0|                     (2,[0],[1.0])|\n",
            "|  red|     0.0|                     (2,[0],[1.0])|\n",
            "|  red|     0.0|                     (2,[0],[1.0])|\n",
            "+-----+--------+----------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
        "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
        "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
        "ohe = ohe.fit(colorLab)\n",
        "ohe.transform(colorLab).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGDWsnNLRX5m"
      },
      "source": [
        "# Text Data Transformers\n",
        "Text is always tricky input because it often requires lots of manipulation to map to a format that\n",
        "a machine learning model will be able to use effectively. There are generally two kinds of texts\n",
        "you’ll see: free-form text and string categorical variables. This section primarily focuses on freeform\n",
        "text because we already discussed categorical variables.\n",
        "\n",
        "## Tokenizing Text\n",
        "Tokenization is the process of converting free-form text into a list of “tokens” or individual\n",
        "words. The easiest way to do this is by using the Tokenizer class. This transformer will take a\n",
        "string of words, separated by whitespace, and convert them into an array of words. For example,\n",
        "in our dataset we might want to convert the Description field into a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjlAPQ5JRX5m",
        "outputId": "812262c5-6d4a-4bdf-bfa7-f8cc7d943f83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+------------------------------------------+\n",
            "|Description                        |DescOut                                   |\n",
            "+-----------------------------------+------------------------------------------+\n",
            "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
            "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
            "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
            "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
            "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
            "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, , water, transfer, tattoos]      |\n",
            "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
            "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
            "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
            "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
            "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
            "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
            "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
            "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, , doorstop, red]         |\n",
            "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
            "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
            "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
            "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
            "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
            "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
            "+-----------------------------------+------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
        "tokenized = tkn.transform(sales.select(\"Description\"))\n",
        "tokenized.show(20, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "931S31LYRX5m"
      },
      "source": [
        "We can also create a Tokenizer that is not just based white space but a regular expression with\n",
        "the RegexTokenizer. The format of the regular expression should conform to the Java Regular\n",
        "Expression (RegEx) syntax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvK-8oVCRX5m",
        "outputId": "ab89d682-8b3e-4170-958c-8885c4ce024f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+------------------------------------------+\n",
            "|Description                        |DescOut                                   |\n",
            "+-----------------------------------+------------------------------------------+\n",
            "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
            "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
            "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
            "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
            "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
            "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, water, transfer, tattoos]        |\n",
            "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
            "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
            "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
            "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
            "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
            "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
            "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
            "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, doorstop, red]           |\n",
            "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
            "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
            "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
            "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
            "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
            "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
            "+-----------------------------------+------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import RegexTokenizer\n",
        "rt = RegexTokenizer()\\\n",
        ".setInputCol(\"Description\")\\\n",
        ".setOutputCol(\"DescOut\")\\\n",
        ".setPattern(\" \")\\\n",
        ".setToLowercase(True)\n",
        "rt.transform(sales.select(\"Description\")).show(20, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq3j2foqRX5n"
      },
      "source": [
        "Another way of using the RegexTokenizer is to use it to output values matching the provided\n",
        "pattern instead of using it as a gap. We do this by setting the gaps parameter to false. Doing this\n",
        "with a space as a pattern returns all the spaces, which is not too useful, but if we made our\n",
        "pattern capture individual words, we could return those:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeTPLMl1RX5n",
        "outputId": "426c25a8-331a-4300-d420-29f720d41278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+------------------+\n",
            "|Description                        |DescOut           |\n",
            "+-----------------------------------+------------------+\n",
            "|RABBIT NIGHT LIGHT                 |[ ,  ]            |\n",
            "|DOUGHNUT LIP GLOSS                 |[ ,  ,  ]         |\n",
            "|12 MESSAGE CARDS WITH ENVELOPES    |[ ,  ,  ,  ]      |\n",
            "|BLUE HARMONICA IN BOX              |[ ,  ,  ,  ]      |\n",
            "|GUMBALL COAT RACK                  |[ ,  ]            |\n",
            "|SKULLS  WATER TRANSFER TATTOOS     |[ ,  ,  ,  ,  ]   |\n",
            "|FELTCRAFT GIRL AMELIE KIT          |[ ,  ,  ]         |\n",
            "|CAMOUFLAGE LED TORCH               |[ ,  ]            |\n",
            "|WHITE SKULL HOT WATER BOTTLE       |[ ,  ,  ,  ,  ]   |\n",
            "|ENGLISH ROSE HOT WATER BOTTLE      |[ ,  ,  ,  ]      |\n",
            "|HOT WATER BOTTLE KEEP CALM         |[ ,  ,  ,  ]      |\n",
            "|SCOTTIE DOG HOT WATER BOTTLE       |[ ,  ,  ,  ]      |\n",
            "|ROSE CARAVAN DOORSTOP              |[ ,  ]            |\n",
            "|GINGHAM HEART  DOORSTOP RED        |[ ,  ,  ,  ]      |\n",
            "|STORAGE TIN VINTAGE LEAF           |[ ,  ,  ]         |\n",
            "|SET OF 4 KNICK KNACK TINS POPPIES  |[ ,  ,  ,  ,  ,  ]|\n",
            "|POPCORN HOLDER                     |[ ]               |\n",
            "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[ ,  ,  ,  ,  ,  ]|\n",
            "|AIRLINE BAG VINTAGE WORLD CHAMPION |[ ,  ,  ,  ,  ]   |\n",
            "|AIRLINE BAG VINTAGE JET SET BROWN  |[ ,  ,  ,  ,  ]   |\n",
            "+-----------------------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import RegexTokenizer\n",
        "rt = RegexTokenizer()\\\n",
        ".setInputCol(\"Description\")\\\n",
        ".setOutputCol(\"DescOut\")\\\n",
        ".setPattern(\" \")\\\n",
        ".setGaps(False)\\\n",
        ".setToLowercase(True)\n",
        "rt.transform(sales.select(\"Description\")).show(20, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrUaze6DRX5n"
      },
      "source": [
        "## Removing Common Words\n",
        "A common task after tokenization is to filter stop words, common words that are not relevant in\n",
        "many kinds of analysis and should thus be removed. Frequently occurring stop words in English\n",
        "include “the,” “and,” and “but.” Spark contains a list of default stop words you can see by calling\n",
        "the following method, which can be made case insensitive if necessary (as of Spark 2.2,\n",
        "supported languages for stopwords are “danish,” “dutch,” “english,” “finnish,” “french,”\n",
        "“german,” “hungarian,” “italian,” “norwegian,” “portuguese,” “russian,” “spanish,” “swedish,”\n",
        "and “turkish”):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdtcwkoyRX5n",
        "outputId": "645999f4-1a56-43a4-cd2b-2f1430c29de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-------------------------------------+\n",
            "|         Description|             DescOut|StopWordsRemover_50e39c6294c6__output|\n",
            "+--------------------+--------------------+-------------------------------------+\n",
            "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|                 [rabbit, night, l...|\n",
            "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|                 [doughnut, lip, g...|\n",
            "|12 MESSAGE CARDS ...|[12, message, car...|                 [12, message, car...|\n",
            "|BLUE HARMONICA IN...|[blue, harmonica,...|                 [blue, harmonica,...|\n",
            "|   GUMBALL COAT RACK|[gumball, coat, r...|                 [gumball, coat, r...|\n",
            "|SKULLS  WATER TRA...|[skulls, , water,...|                 [skulls, , water,...|\n",
            "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|                 [feltcraft, girl,...|\n",
            "|CAMOUFLAGE LED TORCH|[camouflage, led,...|                 [camouflage, led,...|\n",
            "|WHITE SKULL HOT W...|[white, skull, ho...|                 [white, skull, ho...|\n",
            "|ENGLISH ROSE HOT ...|[english, rose, h...|                 [english, rose, h...|\n",
            "|HOT WATER BOTTLE ...|[hot, water, bott...|                 [hot, water, bott...|\n",
            "|SCOTTIE DOG HOT W...|[scottie, dog, ho...|                 [scottie, dog, ho...|\n",
            "|ROSE CARAVAN DOOR...|[rose, caravan, d...|                 [rose, caravan, d...|\n",
            "|GINGHAM HEART  DO...|[gingham, heart, ...|                 [gingham, heart, ...|\n",
            "|STORAGE TIN VINTA...|[storage, tin, vi...|                 [storage, tin, vi...|\n",
            "|SET OF 4 KNICK KN...|[set, of, 4, knic...|                 [set, 4, knick, k...|\n",
            "|      POPCORN HOLDER|   [popcorn, holder]|                    [popcorn, holder]|\n",
            "|GROW A FLYTRAP OR...|[grow, a, flytrap...|                 [grow, flytrap, s...|\n",
            "|AIRLINE BAG VINTA...|[airline, bag, vi...|                 [airline, bag, vi...|\n",
            "|AIRLINE BAG VINTA...|[airline, bag, vi...|                 [airline, bag, vi...|\n",
            "+--------------------+--------------------+-------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
        "stops = StopWordsRemover()\\\n",
        ".setStopWords(englishStopWords)\\\n",
        ".setInputCol(\"DescOut\")\n",
        "stops.transform(tokenized).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZbrkkkNRX5n"
      },
      "source": [
        "Notice how the word of is removed in the output column. That’s because it’s such a common\n",
        "word that it isn’t relevant to any downstream manipulation and simply adds noise to our dataset.\n",
        "## Creating Word Combinations\n",
        "Tokenizing our strings and filtering stop words leaves us with a clean set of words to use as\n",
        "features. It is often of interest to look at combinations of words, usually by looking at colocated\n",
        "words. Word combinations are technically referred to as n-grams—that is, sequences of words of\n",
        "length n. An n-gram of length 1 is called a unigrams; those of length 2 are called bigrams, and\n",
        "those of length 3 are called trigrams (anything above those are just four-gram, five-gram, etc.),\n",
        "Order matters with n-gram creation, so converting a sentence with three words into bigram\n",
        "representation would result in two bigrams. The goal when creating n-grams is to better capture\n",
        "sentence structure and more information than can be gleaned by simply looking at all words\n",
        "individually. Let’s create some n-grams to illustrate this concept.\n",
        "\n",
        "With n-grams, we can look at sequences of words that commonly co-occur and use them as\n",
        "inputs to a machine learning algorithm. These can create better features than simply looking at\n",
        "all of the words individually (say, tokenized on a space character):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bl181h1RX5n",
        "outputId": "90031390-e926-4a61-c0ce-c5137cd997ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+--------------------------+\n",
            "|DescOut               |NGram_929e7ae4e6f4__output|\n",
            "+----------------------+--------------------------+\n",
            "|[rabbit, night, light]|[rabbit, night, light]    |\n",
            "|[doughnut, lip, gloss]|[doughnut, lip, gloss]    |\n",
            "+----------------------+--------------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+----------------------+---------------------------+\n",
            "|DescOut               |NGram_0978287bedda__output |\n",
            "+----------------------+---------------------------+\n",
            "|[rabbit, night, light]|[rabbit night, night light]|\n",
            "|[doughnut, lip, gloss]|[doughnut lip, lip gloss]  |\n",
            "+----------------------+---------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import NGram\n",
        "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
        "bigram = NGram().setInputCol(\"DescOut\").setN(2)\n",
        "unigram.transform(tokenized.select(\"DescOut\")).show(2, False)\n",
        "bigram.transform(tokenized.select(\"DescOut\")).show(2, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWvXbhgnRX5n"
      },
      "source": [
        "## Converting Words into Numerical Representations\n",
        "Once you have word features, it’s time to start counting instances of words and word\n",
        "combinations for use in our models. The simplest way is just to include binary counts of a word\n",
        "in a given document (in our case, a row). Essentially, we’re measuring whether or not each row\n",
        "contains a given word. This is a simple way to normalize for document sizes and occurrence\n",
        "counts and get numerical features that allow us to classify documents based on content. In\n",
        "addition, we can count words using a CountVectorizer, or reweigh them according to the\n",
        "prevalence of a given word in all the documents using a TF–IDF transformation (discussed next)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7ut5OmxRX5n",
        "outputId": "befffc5e-7067-4f99-dc67-69605fa54e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
            "|Description                    |DescOut                              |countVec                                     |\n",
            "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
            "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |(500,[150,185,212],[1.0,1.0,1.0])            |\n",
            "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |(500,[462,463,491],[1.0,1.0,1.0])            |\n",
            "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|(500,[35,41,166],[1.0,1.0,1.0])              |\n",
            "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |(500,[10,16,36,352],[1.0,1.0,1.0,1.0])       |\n",
            "|GUMBALL COAT RACK              |[gumball, coat, rack]                |(500,[228,281,407],[1.0,1.0,1.0])            |\n",
            "|SKULLS  WATER TRANSFER TATTOOS |[skulls, , water, transfer, tattoos] |(500,[11,40,133],[1.0,1.0,1.0])              |\n",
            "|FELTCRAFT GIRL AMELIE KIT      |[feltcraft, girl, amelie, kit]       |(500,[60,64,69],[1.0,1.0,1.0])               |\n",
            "|CAMOUFLAGE LED TORCH           |[camouflage, led, torch]             |(500,[264],[1.0])                            |\n",
            "|WHITE SKULL HOT WATER BOTTLE   |[white, skull, hot, water, bottle]   |(500,[15,34,39,40,118],[1.0,1.0,1.0,1.0,1.0])|\n",
            "|ENGLISH ROSE HOT WATER BOTTLE  |[english, rose, hot, water, bottle]  |(500,[34,39,40,46,169],[1.0,1.0,1.0,1.0,1.0])|\n",
            "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "cv = CountVectorizer()\\\n",
        ".setInputCol(\"DescOut\")\\\n",
        ".setOutputCol(\"countVec\")\\\n",
        ".setVocabSize(500)\\\n",
        ".setMinTF(1)\\\n",
        ".setMinDF(2)\n",
        "fittedCV = cv.fit(tokenized)\n",
        "fittedCV.transform(tokenized).show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxzqWmH7RX5o"
      },
      "source": [
        "While the output looks a little complicated, it’s actually just a sparse vector that contains the total\n",
        "vocabulary size, the index of the word in the vocabulary, and then the counts of that particular\n",
        "word:\n",
        "## Term frequency–inverse document frequency\n",
        "Another way to approach the problem of converting text into a numerical representation is to use\n",
        "term frequency–inverse document frequency (TF–IDF). In simplest terms, TF–IDF measures\n",
        "how often a word occurs in each document, weighted according to how many documents that\n",
        "word occurs in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV2wunpaRX5o",
        "outputId": "3d84be83-4b60-48e7-a95f-dd4c389de179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------+\n",
            "|DescOut                                |\n",
            "+---------------------------------------+\n",
            "|[gingham, heart, , doorstop, red]      |\n",
            "|[red, floral, feltcraft, shoulder, bag]|\n",
            "|[alarm, clock, bakelike, red]          |\n",
            "|[pin, cushion, babushka, red]          |\n",
            "|[red, retrospot, mini, cases]          |\n",
            "|[red, kitchen, scales]                 |\n",
            "|[gingham, heart, , doorstop, red]      |\n",
            "|[large, red, babushka, notebook]       |\n",
            "|[red, retrospot, oven, glove]          |\n",
            "|[red, retrospot, plate]                |\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tfIdfIn = tokenized\\\n",
        ".where(\"array_contains(DescOut, 'red')\")\\\n",
        ".select(\"DescOut\")\\\n",
        ".limit(10)\n",
        "tfIdfIn.show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCPkkqptRX5o"
      },
      "source": [
        "We can see some overlapping words in these documents, but these words provide at least a rough\n",
        "topic-like representation. Now let’s input that into TF–IDF. To do this, we’re going to hash each\n",
        "word and convert it to a numerical representation, and then weigh each word in the voculary\n",
        "according to the inverse document frequency. Hashing is a similar process as CountVectorizer,\n",
        "but is irreversible—that is, from our output index for a word, we cannot get our input word\n",
        "(multiple words might map to the same output index):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dmv3bUi5RX5o"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "tf = HashingTF()\\\n",
        ".setInputCol(\"DescOut\")\\\n",
        ".setOutputCol(\"TFOut\")\\\n",
        ".setNumFeatures(10000)\n",
        "idf = IDF()\\\n",
        ".setInputCol(\"TFOut\")\\\n",
        ".setOutputCol(\"IDFOut\")\\\n",
        ".setMinDocFreq(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsHCgnASRX5o",
        "outputId": "89ff36da-a36b-4a29-ec57-32c19b4ab105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
            "|DescOut                                |TFOut                                                |IDFOut                                                                                                           |\n",
            "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
            "|[gingham, heart, , doorstop, red]      |(10000,[52,804,3372,6594,9808],[1.0,1.0,1.0,1.0,1.0])|(10000,[52,804,3372,6594,9808],[0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
            "|[red, floral, feltcraft, shoulder, bag]|(10000,[50,52,415,6756,8005],[1.0,1.0,1.0,1.0,1.0])  |(10000,[50,52,415,6756,8005],[0.0,0.0,0.0,0.0,0.0])                                                              |\n",
            "|[alarm, clock, bakelike, red]          |(10000,[52,4995,8737,9001],[1.0,1.0,1.0,1.0])        |(10000,[52,4995,8737,9001],[0.0,0.0,0.0,0.0])                                                                    |\n",
            "|[pin, cushion, babushka, red]          |(10000,[52,610,2490,7153],[1.0,1.0,1.0,1.0])         |(10000,[52,610,2490,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
            "|[red, retrospot, mini, cases]          |(10000,[52,547,6703,8448],[1.0,1.0,1.0,1.0])         |(10000,[52,547,6703,8448],[0.0,0.0,0.0,1.0116009116784799])                                                      |\n",
            "|[red, kitchen, scales]                 |(10000,[52,756,6452],[1.0,1.0,1.0])                  |(10000,[52,756,6452],[0.0,0.0,0.0])                                                                              |\n",
            "|[gingham, heart, , doorstop, red]      |(10000,[52,804,3372,6594,9808],[1.0,1.0,1.0,1.0,1.0])|(10000,[52,804,3372,6594,9808],[0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
            "|[large, red, babushka, notebook]       |(10000,[52,2787,7022,7153],[1.0,1.0,1.0,1.0])        |(10000,[52,2787,7022,7153],[0.0,0.0,0.0,1.2992829841302609])                                                     |\n",
            "|[red, retrospot, oven, glove]          |(10000,[52,8242,8448,8667],[1.0,1.0,1.0,1.0])        |(10000,[52,8242,8448,8667],[0.0,0.0,1.0116009116784799,0.0])                                                     |\n",
            "|[red, retrospot, plate]                |(10000,[52,4925,8448],[1.0,1.0,1.0])                 |(10000,[52,4925,8448],[0.0,0.0,1.0116009116784799])                                                              |\n",
            "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L7TxuNQRX5p"
      },
      "source": [
        "(10000,[2591,4291,4456],[1.0116009116784799,0.0,0.0])\n",
        "\n",
        "This vector is represented using three different values: the total vocabulary size, the hash of\n",
        "every word appearing in the document, and the weighting of each of those terms. This is similar\n",
        "to the CountVectorizer output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUmpmSyRX5p"
      },
      "source": [
        "## Word2Vec\n",
        "Word2Vec is a deep learning–based tool for computing a vector representation of a set of words.\n",
        "The goal is to have similar words close to one another in this vector space, so we can then make\n",
        "generalizations about the words themselves. This model is easy to train and use, and has been\n",
        "shown to be useful in a number of natural language processing applications, including entity\n",
        "recognition, disambiguation, parsing, tagging, and machine translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vULouBvURX5p",
        "outputId": "d097e9dd-d0ec-4628-90bf-b0b6f15d94e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: [Hi, I, heard, about, Spark] => \n",
            "Vector: [0.0602305562235415,-0.03974485695362091,-0.02086082473397255]\n",
            "\n",
            "Text: [I, wish, Java, could, use, case, classes] => \n",
            "Vector: [0.05173435062170029,0.0684281130587416,-0.025289185611265044]\n",
            "\n",
            "Text: [Logistic, regression, models, are, neat] => \n",
            "Vector: [0.040317742526531225,0.020868362882174554,0.010597124695777893]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "# Input data: Each row is a bag of words from a sentence or document.\n",
        "documentDF = spark.createDataFrame([\n",
        "(\"Hi I heard about Spark\".split(\" \"), ),\n",
        "(\"I wish Java could use case classes\".split(\" \"), ),\n",
        "(\"Logistic regression models are neat\".split(\" \"), )\n",
        "], [\"text\"])\n",
        "# Learn a mapping from words to Vectors.\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\",\n",
        "outputCol=\"result\")\n",
        "model = word2Vec.fit(documentDF)\n",
        "result = model.transform(documentDF)\n",
        "for row in result.collect():\n",
        "    text, vector = row\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA7hLqYORX5q"
      },
      "source": [
        "# Feature Manipulation\n",
        "While nearly every transformer in ML manipulates the feature space in some way, the following\n",
        "algorithms and tools are automated means of either expanding the input feature vectors or\n",
        "reducing them to a lower number of dimensions.\n",
        "\n",
        "## PCA\n",
        "Principal Components Analysis (PCA) is a mathematical technique for finding the most\n",
        "important aspects of our data (the principal components). It changes the feature representation of\n",
        "our data by creating a new set of features (“aspects”). Each new feature is a combination of the\n",
        "original features. The power of PCA is that it can create a smaller set of more meaningful\n",
        "features to be input into your model, at the potential cost of interpretability.\n",
        "\n",
        "You’d want to use PCA if you have a large input dataset and want to reduce the total number of\n",
        "features you have. This frequently comes up in text analysis where the entire feature space is\n",
        "massive and many of the features are largely irrelevant. Using PCA, we can find the most\n",
        "important combinations of features and only include those in our machine learning model. PCA\n",
        "takes a parameter 􁽅, specifying the number of output features to create. Generally, this should be\n",
        "much smaller than your input vectors’ dimension.\n",
        "\n",
        "### NOTE\n",
        "Picking the right 􁽅 is nontrivial and there’s no prescription we can give. Check out the relevant\n",
        "chapters in ESL and ISL for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAGARNzVRX5q",
        "outputId": "bcd128a1-fa73-443c-f37b-12294bc72e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+------------------------------------------+\n",
            "|id |features      |PCA_ca76290d226c__output                  |\n",
            "+---+--------------+------------------------------------------+\n",
            "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.45266548881478363] |\n",
            "|1  |[2.0,1.1,1.0] |[-1.680494698407372,1.259340132221916]    |\n",
            "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.45266548881478363] |\n",
            "|1  |[2.0,1.1,1.0] |[-1.680494698407372,1.259340132221916]    |\n",
            "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060152422]|\n",
            "+---+--------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "pca = PCA().setInputCol(\"features\").setK(2)\n",
        "pca.fit(scaleDF).transform(scaleDF).show(20, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGkuak3fRX5q"
      },
      "source": [
        "## Interaction\n",
        "In some cases, you might have domain knowledge about specific variables in your dataset. For\n",
        "example, you might know that a certain interaction between the two variables is an important\n",
        "variable to include in a downstream estimator. The feature transformer Interaction allows you\n",
        "to create an interaction between two variables manually. It just multiplies the two features\n",
        "together—something that a typical linear model would not do for every possible pair of features\n",
        "in your data. This transformer is currently only available directly in Scala but can be called from\n",
        "any language using the RFormula. We recommend users just use RFormula instead of manually\n",
        "creating interactions.\n",
        "## Polynomial Expansion\n",
        "Polynomial expansion is used to generate interaction variables of all the input columns. With\n",
        "polynomial expansion, we specify to what degree we would like to see various interactions. For\n",
        "example, for a degree-2 polynomial, Spark takes every value in our feature vector, multiplies it\n",
        "by every other value in the feature vector, and then stores the results as features. For instance, if\n",
        "we have two input features, we’ll get four output features if we use a second degree polynomial\n",
        "(2x2). If we have three input features, we’ll get nine output features (3x3). If we use a thirddegree\n",
        "polynomial, we’ll get 27 output features (3x3x3) and so on. This transformation is useful\n",
        "when you want to see interactions between particular features but aren’t necessarily sure about\n",
        "which interactions to consider.\n",
        "\n",
        "### WARNING\n",
        "Polynomial expansion can greatly increase your feature space, leading to both high computational\n",
        "costs and overfitting. Use it with caution, especially for higher degrees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tpCV03vRX5q",
        "outputId": "41c5d91b-ee4e-43b7-aa68-fcc6001e1856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+----------------------------------------+\n",
            "| id|      features|PolynomialExpansion_6ee9be80c196__output|\n",
            "+---+--------------+----------------------------------------+\n",
            "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
            "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
            "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
            "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
            "|  1|[3.0,10.1,3.0]|                    [3.0,9.0,10.1,30....|\n",
            "+---+--------------+----------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import PolynomialExpansion\n",
        "pe = PolynomialExpansion().setInputCol(\"features\").setDegree(2)\n",
        "pe.transform(scaleDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6O1eHReRX5q"
      },
      "source": [
        "# Feature Selection\n",
        "Often, you will have a large range of possible features and want to select a smaller subset to use\n",
        "for training. For example, many features might be correlated, or using too many features might\n",
        "lead to overfitting. This process is called feature selection. There are a number of ways to\n",
        "evaluate feature importance once you’ve trained a model but another option is to do some rough\n",
        "filtering beforehand. Spark has some simple options for doing that, such as ChiSqSelector.\n",
        "## ChiSqSelector\n",
        "ChiSqSelector leverages a statistical test to identify features that are not independent from the\n",
        "label we are trying to predict, and drop the uncorrelated features. It’s often used with categorical\n",
        "data in order to reduce the number of features you will input into your model, as well as to\n",
        "reduce the dimensionality of text data (in the form of frequencies or counts). Since this method is\n",
        "based on the Chi-Square test, there are several different ways we can pick the “best” features.\n",
        "The methods are numTopFeatures, which is ordered by p-value; percentile, which takes a\n",
        "proportion of the input features (instead of just the top N features); and fpr, which sets a cut off\n",
        "p-value.\n",
        "\n",
        "We will demonstrate this with the output of the CountVectorizer created earlier in this chapter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N79zZbdRX5q",
        "outputId": "155381c4-ea6a-4a8d-f4ec-a11a7219bdf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------------------------+\n",
            "|            countVec|ChiSqSelector_155067f22bc8__output|\n",
            "+--------------------+----------------------------------+\n",
            "|(500,[150,185,212...|                         (2,[],[])|\n",
            "|(500,[462,463,491...|                         (2,[],[])|\n",
            "|(500,[35,41,166],...|                         (2,[],[])|\n",
            "|(500,[10,16,36,35...|                         (2,[],[])|\n",
            "|(500,[228,281,407...|                         (2,[],[])|\n",
            "|(500,[11,40,133],...|                         (2,[],[])|\n",
            "|(500,[60,64,69],[...|                         (2,[],[])|\n",
            "|   (500,[264],[1.0])|                         (2,[],[])|\n",
            "|(500,[15,34,39,40...|                         (2,[],[])|\n",
            "|(500,[34,39,40,46...|                         (2,[],[])|\n",
            "|(500,[34,39,40,14...|                         (2,[],[])|\n",
            "|(500,[34,39,40,14...|                         (2,[],[])|\n",
            "|(500,[46,297],[1....|                         (2,[],[])|\n",
            "|(500,[3,4,11,143,...|                         (2,[],[])|\n",
            "|(500,[6,45,109,16...|                         (2,[],[])|\n",
            "|(500,[0,1,49,70,3...|               (2,[0,1],[1.0,1.0])|\n",
            "|(500,[21,296],[1....|                         (2,[],[])|\n",
            "|(500,[36,45,378],...|                         (2,[],[])|\n",
            "|(500,[2,6,328],[1...|                         (2,[],[])|\n",
            "|(500,[0,2,6,328,4...|                     (2,[0],[1.0])|\n",
            "+--------------------+----------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
        "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
        "tokenized = tkn\\\n",
        ".transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
        ".where(\"CustomerId IS NOT NULL\")\n",
        "prechi = fittedCV.transform(tokenized)\\\n",
        ".where(\"CustomerId IS NOT NULL\")\n",
        "chisq = ChiSqSelector()\\\n",
        ".setFeaturesCol(\"countVec\")\\\n",
        ".setLabelCol(\"CustomerId\")\\\n",
        ".setNumTopFeatures(2)\n",
        "\n",
        "chisq.fit(prechi).transform(prechi)\\\n",
        ".drop(\"customerId\", \"Description\", \"DescOut\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1iQOOH0RX5q"
      },
      "source": [
        "# Advanced Topics\n",
        "There are several advanced topics surrounding transformers and estimators. Here we touch on\n",
        "the two most common, persisting transformers as well as writing custom ones.\n",
        "## Persisting Transformers\n",
        "Once you’ve used an estimator to configure a transformer, it can be helpful to write it to disk and\n",
        "simply load it when necessary (e.g., for use in another Spark session). We saw this in the\n",
        "previous chapter when we persisted an entire pipeline. To persist a transformer individually, we\n",
        "use the write method on the fitted transformer (or the standard transformer) and specify the\n",
        "location:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SWH5qS4uRX5q"
      },
      "outputs": [],
      "source": [
        "fittedPCA = pca.fit(scaleDF)\n",
        "fittedPCA.write().overwrite().save(\"/tmp/fittedPCA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vIsJkG4RX5q"
      },
      "source": [
        "We can then load it back in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAjHU00vRX5r",
        "outputId": "8ed32e30-4d31-46a8-fdd4-730acca682a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+-----------------------------+\n",
            "| id|      features|PCAModel_863690777fc4__output|\n",
            "+---+--------------+-----------------------------+\n",
            "|  0|[1.0,0.1,-1.0]|         [0.07137194992484...|\n",
            "|  1| [2.0,1.1,1.0]|         [-1.6804946984073...|\n",
            "|  0|[1.0,0.1,-1.0]|         [0.07137194992484...|\n",
            "|  1| [2.0,1.1,1.0]|         [-1.6804946984073...|\n",
            "|  1|[3.0,10.1,3.0]|         [-10.872398139848...|\n",
            "+---+--------------+-----------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import PCAModel\n",
        "loadedPCA = PCAModel.load(\"/tmp/fittedPCA\")\n",
        "loadedPCA.transform(scaleDF).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEAJn7ErRX5r"
      },
      "source": [
        "# Conclusion\n",
        "This chapter gave a whirlwind tour of many of the most common preprocessing transformations\n",
        "Spark has available. There are several domain-specific ones we did not have enough room to\n",
        "cover (e.g., Discrete Cosine Transform), but you can find more information in the\n",
        "documentation. This area of Spark is also constantly growing as the community develops new\n",
        "ones.\n",
        "\n",
        "Another important aspect of this feature engineering toolkit is consistency. In the previous\n",
        "chapter we covered the pipeline concept, an essential tool to package and train end-to-end ML\n",
        "workflows. In the next chapter we will start going through the variety of machine learning tasks you may have and what algorithms are available for each one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5QQ4x4NRX5r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}