{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDTOm0QmbL4c"
      },
      "source": [
        "# Types of Classification\n",
        "Before we continue, let’s review several different types of classification.\n",
        "## Binary Classification\n",
        "The simplest example of classification is binary classification, where there are only two labels\n",
        "you can predict. One example is fraud analytics, where a given transaction can be classified as\n",
        "fraudulent or not; or email spam, where a given email can be classified as spam or not spam.\n",
        "## Multiclass Classification\n",
        "Beyond binary classification lies multiclass classification, where one label is chosen from more\n",
        "than two distinct possible labels. A typical example is Facebook predicting the people in a given\n",
        "photo or a meterologist predicting the weather (rainy, sunny, cloudy, etc.). Note how there is\n",
        "always a finite set of classes to predict; it’s never unbounded. This is also called multinomial\n",
        "classification.\n",
        "## Multilabel Classification\n",
        "Finally, there is multilabel classification, where a given input can produce multiple labels. For\n",
        "example, you might want to predict a book’s genre based on the text of the book itself. While\n",
        "this could be multiclass, it’s probably better suited for multilabel because a book may fall into\n",
        "multiple genres. Another example of multilabel classification is identifying the number of objects\n",
        "that appear in an image. Note that in this example, the number of output predictions is not\n",
        "necessarily fixed, and could vary from image to image.\n",
        "\n",
        "# Model Scalability\n",
        "Model scalability is an important consideration when choosing your model. In general, Spark has\n",
        "great support for training large-scale machine learning models (note, these are large scale; on\n",
        "single-node workloads there are a number of other tools that also perform well). Table 26-1 is a\n",
        "simple model scalability scorecard to use to find the best model for your particular task (if\n",
        "scalability is your core consideration). The actual scalability will depend on your configuration,\n",
        "machine size, and other specifics but should make for a good heuristic.\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LuKhxcT-bL4e"
      },
      "outputs": [],
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRkAk_dzbiu-",
        "outputId": "f73c9e58-7baf-4e93-b6f2-97e48d199cc4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jf7PMy0-bL4e"
      },
      "outputs": [],
      "source": [
        "bInput = spark.read.format(\"parquet\").load(\"drive/MyDrive/UET/BigData/Lab07/data/binary-classification\")\\\n",
        ".selectExpr(\"features\", \"cast(label as double) as label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4olfEnMAbL4e"
      },
      "source": [
        "## Logistic Regression\n",
        "Logistic regression is one of the most popular methods of classification. It is a linear method that\n",
        "combines each of the individual inputs (or features) with specific weights (these weights are\n",
        "generated during the training process) that are then combined to get a probability of belonging to\n",
        "a particular class. These weights are helpful because they are good representations of feature\n",
        "importance; if you have a large weight, you can assume that variations in that feature have a\n",
        "significant effect on the outcome (assuming you performed normalization). A smaller weight\n",
        "means the feature is less likely to be important.\n",
        "\n",
        "Here’s a simple example using the LogisticRegression model. Notice how we didn’t specify any\n",
        "parameters because we’ll leverage the defaults and our data conforms to the proper column\n",
        "naming. In practice, you probably won’t need to change many of the parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug2SVJA8bL4e",
        "outputId": "d256dbe1-af3b-4b20-ab34-1dca27684653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
            "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
            "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
            "featuresCol: features column name. (default: features)\n",
            "fitIntercept: whether to fit an intercept term. (default: True)\n",
            "labelCol: label column name. (default: label)\n",
            "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
            "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
            "maxIter: max number of iterations (>= 0). (default: 100)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "regParam: regularization parameter (>= 0). (default: 0.0)\n",
            "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
            "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
            "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "print(lr.explainParams()) # see all parameters\n",
        "lrModel = lr.fit(bInput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5eUE0SnbL4f"
      },
      "source": [
        "Once the model is trained you can get information about the model by taking a look at the\n",
        "coefficients and the intercept. The coefficients correspond to the individual feature weights (each\n",
        "feature weight is multiplied by each respective feature to compute the prediction) while the\n",
        "intercept is the value of the italics-intercept (if we chose to fit one when specifying the model).\n",
        "Seeing the coefficients can be helpful for inspecting the model that you built and comparing how\n",
        "features affect the prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Yrl4eg1bL4f",
        "outputId": "c4015666-e3d6-4710-ee11-4d8801d53103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18.722385741661302,-0.5693688557340782,9.361192870830642]\n",
            "-28.04329511868947\n"
          ]
        }
      ],
      "source": [
        "print(lrModel.coefficients)\n",
        "print(lrModel.intercept)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_aWIikqbL4f"
      },
      "source": [
        "### Model Summary\n",
        "Logistic regression provides a model summary that gives you information about the final, trained\n",
        "model. This is analogous to the same types of summaries we see in many R language machine\n",
        "learning packages. The model summary is currently only available for binary logistic regression\n",
        "problems, but multiclass summaries will likely be added in the future. Using the binary\n",
        "summary, we can get all sorts of information about the model itself including the area under the\n",
        "ROC curve, the f measure by threshold, the precision, the recall, the recall by thresholds, and the\n",
        "ROC curve. Note that for the area under the curve, instance weighting is not taken into account,\n",
        "so if you wanted to see how you performed on the values you weighed more highly, you’d have\n",
        "to do that manually. This will probably change in future Spark versions. You can see the\n",
        "summary using the following APIs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayu-FVxpbL4f",
        "outputId": "9964a5f1-8aeb-4f26-f5ed-b4ef807efe25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "+---+------------------+\n",
            "|FPR|               TPR|\n",
            "+---+------------------+\n",
            "|0.0|               0.0|\n",
            "|0.0|0.3333333333333333|\n",
            "|0.0|               1.0|\n",
            "|1.0|               1.0|\n",
            "|1.0|               1.0|\n",
            "+---+------------------+\n",
            "\n",
            "+------------------+---------+\n",
            "|            recall|precision|\n",
            "+------------------+---------+\n",
            "|               0.0|      1.0|\n",
            "|0.3333333333333333|      1.0|\n",
            "|               1.0|      1.0|\n",
            "|               1.0|      0.6|\n",
            "+------------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "summary = lrModel.summary\n",
        "print(summary.areaUnderROC)\n",
        "summary.roc.show()\n",
        "summary.pr.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPN69SZpbL4f"
      },
      "source": [
        "The speed at which the model descends to the final result is shown in the objective history. We\n",
        "can access this through the objective history on the model summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHy8W52ZbL4g",
        "outputId": "0a881b02-a5bd-4663-f996-9c005c543789"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6730116670092563,\n",
              " 0.30533476678669746,\n",
              " 0.19572951692227342,\n",
              " 0.08238560717506735,\n",
              " 0.039904390712412516,\n",
              " 0.019187605729977825,\n",
              " 0.009480513129879648,\n",
              " 0.00470079397539893,\n",
              " 0.0023428240050888194,\n",
              " 0.0011692212872630964,\n",
              " 0.000584133352645369,\n",
              " 0.000291938436814461,\n",
              " 0.00014593757317782482,\n",
              " 7.295887614374282e-05,\n",
              " 3.647309882223256e-05,\n",
              " 1.8228017083424128e-05,\n",
              " 9.095755464927068e-06,\n",
              " 4.5053062928456136e-06,\n",
              " 2.17434840951629e-06,\n",
              " 1.0422594942126292e-06,\n",
              " 5.280808738948523e-07,\n",
              " 2.628531186444607e-07,\n",
              " 1.3166032239693547e-07,\n",
              " 6.578498712561095e-08,\n",
              " 3.290121373800949e-08,\n",
              " 1.6448921648782466e-08,\n",
              " 8.224786126081387e-09]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "summary.objectiveHistory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRndzCqbL4g"
      },
      "source": [
        "This is an array of doubles that specify how, over each training iteration, we are performing with\n",
        "respect to our objective function. This information is helpful to see if we have sufficient\n",
        "iterations or need to be tuning other parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYZ3X_G9bL4g"
      },
      "source": [
        "# Decision Trees\n",
        "Decision trees are one of the more friendly and interpretable models for performing classification\n",
        "because they’re similar to simple decision models that humans use quite often. For example, if\n",
        "you have to predict whether or not someone will eat ice cream when offered, a good feature\n",
        "might be whether or not that individual likes ice cream. In pseudocode, if\n",
        "person.likes(“ice_cream”), they will eat ice cream; otherwise, they won’t eat ice cream. A\n",
        "decision tree creates this type of structure with all the inputs and follows a set of branches when\n",
        "it comes time to make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaLBy4aAbL4g",
        "outputId": "907a2c4e-0e5a-4f9b-a493-1c136e26be9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
            "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
            "featuresCol: features column name. (default: features)\n",
            "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
            "labelCol: label column name. (default: label)\n",
            "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
            "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\n",
            "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
            "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
            "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
            "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: -1093424464674917824)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "print(dt.explainParams())\n",
        "dtModel = dt.fit(bInput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg9B1t2wbL4g"
      },
      "source": [
        "# Random Forest and Gradient-Boosted Trees\n",
        "These methods are extensions of the decision tree. Rather than training one tree on all of the data,\n",
        "you train multiple trees on varying subsets of the data. The intuition behind doing this is that\n",
        "various decision trees will become “experts” in that particular domain while others become\n",
        "experts in others. By combining these various experts, you then get a “wisdom of the crowds”\n",
        "effect, where the group’s performance exceeds any individual. In addition, these methods can\n",
        "help prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuZ9sWNFbL4g",
        "outputId": "9dbbcfb3-6ad2-40ee-af04-531b51367b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bootstrap: Whether bootstrap samples are used when building trees. (default: True)\n",
            "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
            "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
            "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
            "featuresCol: features column name. (default: features)\n",
            "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
            "labelCol: label column name. (default: label)\n",
            "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
            "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\n",
            "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
            "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
            "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
            "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
            "numTrees: Number of trees to train (>= 1). (default: 20)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: 1658895934661042375)\n",
            "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
            "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
            "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
            "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: all)\n",
            "featuresCol: features column name. (default: features)\n",
            "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
            "labelCol: label column name. (default: label)\n",
            "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
            "lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic (default: logistic)\n",
            "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\n",
            "maxIter: max number of iterations (>= 0). (default: 20)\n",
            "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
            "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
            "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
            "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: 1582410527311929519)\n",
            "stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)\n",
            "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "validationIndicatorCol: name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation. (undefined)\n",
            "validationTol: Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used. (default: 0.01)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
          ]
        }
      ],
      "source": [
        "# in Python\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rfClassifier = RandomForestClassifier()\n",
        "print(rfClassifier.explainParams())\n",
        "trainedModel = rfClassifier.fit(bInput)\n",
        "# in Python\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbtClassifier = GBTClassifier()\n",
        "print(gbtClassifier.explainParams())\n",
        "trainedModel = gbtClassifier.fit(bInput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6Fh6G1jbL4g"
      },
      "source": [
        "# Evaluators for Classification and Automating Model Tuning\n",
        "As we saw in Chapter 24, evaluators allow us to specify the metric of success for our model. An\n",
        "evaluator doesn’t help too much when it stands alone; however, when we use it in a pipeline, we\n",
        "can automate a grid search of our various parameters of the models and transformers—trying all\n",
        "combinations of the parameters to see which ones perform the best. Evaluators are most useful in\n",
        "this pipeline and parameter grid context. For classification, there are two evaluators, and they\n",
        "expect two columns: a predicted label from the model and a true label. For binary classification\n",
        "we use the BinaryClassificationEvaluator. This supports optimizing for two different\n",
        "metrics “areaUnderROC” and areaUnderPR.” For multiclass classification, we need to use the\n",
        "MulticlassClassificationEvaluator, which supports optimizing for “f1”,\n",
        "“weightedPrecision”, “weightedRecall”, and “accuracy”.\n",
        "To use evaluators, we build up our pipeline, specify the parameters we would like to test, and\n",
        "then run it and see the results. See Chapter 24 for a code example.\n",
        "\n",
        "There are three different classification metrics we can use:\n",
        "1. Binary classification metrics\n",
        "2. Multiclass classification metrics\n",
        "3. Multilabel classification metrics\n",
        "\n",
        "All of these measures follow the same approximate style. We’ll compare generated outputs with\n",
        "true values and the model calculates all of the relevant metrics for us. Then we can query the\n",
        "object for the values for each of the metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NMu5bYtbL4g",
        "outputId": "b72f8bd1-b023-47d7-dfd0-17e54bec104d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "out = trainedModel.transform(bInput)\\\n",
        ".select(\"prediction\", \"label\")\\\n",
        ".rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
        "metrics = BinaryClassificationMetrics(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUVfbUWJbL4g"
      },
      "source": [
        "Once we’ve done that, we can see typical classification success metrics on this metric’s object\n",
        "using a similar API to the one we saw with logistic regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi-XNKYgbL4h",
        "outputId": "14016da2-30d0-43e2-8194-b0bf7883612a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "1.0\n",
            "Receiver Operating Characteristic\n"
          ]
        }
      ],
      "source": [
        "print(metrics.areaUnderPR)\n",
        "print(metrics.areaUnderROC)\n",
        "print(\"Receiver Operating Characteristic\")\n",
        "# metrics.unpersist().toDF().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xJncik6ybL4h",
        "outputId": "32eac033-1870-401c-e8e0-dca9e060c6ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "sc.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2PBPG0ObL4h",
        "outputId": "18b67c4a-1ca8-4f15-a4e9-b67c87f5eb54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__del__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_java_model',\n",
              " '_sc',\n",
              " 'areaUnderPR',\n",
              " 'areaUnderROC',\n",
              " 'call',\n",
              " 'unpersist']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "dir(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skyTqedjbL4h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}